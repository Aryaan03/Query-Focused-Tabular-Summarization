{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "795642bf-b273-4100-9b0d-60e7f8a75252",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#%pip install accelerate peft bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dd8d771-dbd3-4c90-9f12-1ad3eb7bcac3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch \n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92c11fd4-3892-45ed-8c85-64dc6752a290",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from typing import List, Dict\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import evaluate\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efb8d1b3-a7ce-45f9-98b6-0906e02b982f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 500/500 [00:01<00:00, 257.22 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['row_ids', 'table', 'summary', 'query', 'example_id', 'coordinates', 'answers'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = load_from_disk(\"/home/y.khan/cai6307-y.khan/Query-Focused-Tabular-Summarization/data/decomposed/decomposed_train\")\n",
    "test_df = load_from_disk(\"/home/y.khan/cai6307-y.khan/Query-Focused-Tabular-Summarization/data/decomposed/decomposed_test\")\n",
    "validate_df = load_from_disk(\"/home/y.khan/cai6307-y.khan/Query-Focused-Tabular-Summarization/data/decomposed/decomposed_validate\")\n",
    "test_example_ids = set(test_df['example_id'])\n",
    "validate_example_ids = set(validate_df['example_id'])\n",
    "common_example_ids = test_example_ids.intersection(validate_example_ids)\n",
    "\n",
    "test_df = test_df.filter(lambda example: example['example_id'] not in common_example_ids)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80846179-6c33-4751-9abd-bdfc5b6c46bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_training_prompt(query: str, table: str, summary: str, system_prompt: str):\n",
    "    \n",
    "    return f\"\"\"### Instruction: {system_prompt}\n",
    "    \\n\\n### Input: \n",
    "Table: {table.strip()}\n",
    "Query: {query.strip()} \\n\\n\n",
    "### Summary: {summary}\"\"\".strip()\n",
    "\n",
    "def flatten_table(table: Dict) -> str:\n",
    "    header = table.get('header', [])\n",
    "    rows = table.get('rows', [])\n",
    "    title = table.get('title', [])\n",
    "\n",
    "    flattened_rows = []\n",
    "\n",
    "    for i, row in enumerate(rows):\n",
    "        row_text = f\"Row {i}, \" + \",\".join([f\"{col}:{val}\" for col, val in zip(header, row)])\n",
    "        flattened_rows.append(\"## \" + row_text)\n",
    "\n",
    "    flattened_table = f\"Title: {' '.join(map(str, title))}\" + \" \" + \" \".join(flattened_rows)\n",
    "\n",
    "    return flattened_table\n",
    "\n",
    "\n",
    "def generate_instruction_dataset(data_point):\n",
    "    \n",
    "    task_prefix = \"Given a table and a query, generate a summary that answers the query based on the information in the table: \"\n",
    "\n",
    "    return {\n",
    "        \"query\": data_point[\"query\"],\n",
    "        \"table\": flatten_table(data_point[\"table\"]),\n",
    "        \"summary\": data_point[\"summary\"],\n",
    "        \"text\": generate_training_prompt(data_point[\"query\"], flatten_table(data_point[\"table\"]),  data_point['summary'], task_prefix)\n",
    "    }\n",
    "\n",
    "def process_dataset(data: Dataset):\n",
    "    return (\n",
    "        data.shuffle(seed=42)\n",
    "        .map(generate_instruction_dataset).remove_columns(['row_ids', 'example_id', 'coordinates', 'answers']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e47af59-690e-4988-b87d-dfb4da1f95f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 300/300 [00:00<00:00, 677.16 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['table', 'summary', 'query', 'text'],\n",
       "     num_rows: 1800\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['table', 'summary', 'query', 'text'],\n",
       "     num_rows: 300\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['table', 'summary', 'query', 'text'],\n",
       "     num_rows: 200\n",
       " }))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## APPLYING PREPROCESSING ON WHOLE DATASET\n",
    "train_df = process_dataset(train_df)\n",
    "validate_df = process_dataset(validate_df)\n",
    "test_df = process_dataset(test_df)\n",
    "\n",
    "# Select 1000 rows from the training split\n",
    "train_data = train_df.shuffle(seed=42).select([i for i in range(1800)])\n",
    "\n",
    "\n",
    "test_data = test_df.shuffle(seed=42).select([i for i in range(300)])\n",
    "validation_data = validate_df\n",
    "\n",
    "train_data,test_data,validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec055bfb-8323-43f5-870d-04d7e9f940aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.00s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "cache_dir = \"./llama3-cache\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B\"\n",
    "#base_model = \"llama-2-7b-QTsumm\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    #quantization_config=quant_config,\n",
    "    token=\"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\",\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True, \n",
    "                                          token=\"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\",\n",
    "                                          cache_dir=cache_dir\n",
    "                                         )\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b1b1da-c44f-467e-a325-d0064ec46040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "index = 21\n",
    "\n",
    "query = test_df['query'][index]\n",
    "table = test_df['table'][index]\n",
    "summary = test_df['summary'][index]\n",
    "task_prefix = \"Given a query and a table, generate a summary that answers the query based on the information in the table: \"\n",
    "\n",
    "prompt = f\"\"\"### Instruction: {task_prefix}\n",
    "    \\n\\n### Input: \n",
    "Table: {table.strip()}\n",
    "Query: {query.strip()} \\n\\n\n",
    "### Summary: \"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(model.generate(inputs[\"input_ids\"], max_new_tokens=100)[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d137db2f-ae5e-44ab-b7b0-383886a0119d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_r = 16\n",
    "lora_alpha = 64\n",
    "lora_dropout = 0.05\n",
    "lora_target_modules = [\"q_proj\", \"up_proj\", \"o_proj\", \"k_proj\", \"down_proj\", \"gate_proj\",\"v_proj\", \"ln\", \"fc\"]\n",
    "\n",
    "\n",
    "peft_params = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=lora_target_modules,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0af37b4d-d208-4a7b-bc78-94f76d3866b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 300/300 [00:00<00:00, 756.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "        preds = [pred.strip() for pred in preds]\n",
    "        labels = [label.strip() for label in labels]\n",
    "\n",
    "        # rougeLSum expects newline after each sentence\n",
    "        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "        return preds, labels\n",
    "\n",
    "def metric_fn(eval_predictions):\n",
    "    predictions, labels = eval_predictions\n",
    "    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    for label in labels:\n",
    "        label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    decoded_predictions, decoded_labels = postprocess_text(decoded_predictions, decoded_labels)\n",
    "\n",
    "    rouge = evaluate.load('rouge')\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    rouge_results = rouge.compute(predictions=decoded_predictions, references=decoded_labels)\n",
    "\n",
    "    return rouge_results\n",
    "\n",
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./train_weights_8b\",\n",
    "    save_strategy = \"no\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=-1,\n",
    "    logging_steps=50,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.003,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.05,\n",
    "    group_by_length=True,\n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    peft_config=peft_params,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=900,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    packing=False,\n",
    "    compute_metrics=metric_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3e1e89a-c871-4771-9f1f-b01e07f44804",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1120' max='1120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1120/1120 10:31:20, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.689400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.523300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.404300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.307300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.169900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.120500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.095400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.075300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.062400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.044200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.043200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.039700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.039600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.034300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.033800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.033900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1120, training_loss=0.1414074673716511, metrics={'train_runtime': 37958.0762, 'train_samples_per_second': 0.948, 'train_steps_per_second': 0.03, 'total_flos': 8.894065708512707e+17, 'train_loss': 0.1414074673716511, 'epoch': 19.91})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee440b10-9094-4bdb-b4db-598731dc1eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('llama-3-8b/tokenizer_config.json',\n",
       " 'llama-3-8b/special_tokens_map.json',\n",
       " 'llama-3-8b/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = \"llama-3-8b\"\n",
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a66dc9-ac66-473b-b1ab-43857f9fd8f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e608ffff-d0e4-4eb6-a084-28eeaabbb820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validate_df_size = len(validation_data)\n",
    "step_size = 3\n",
    "num_batches = validate_df_size // step_size\n",
    "\n",
    "valid = []\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start_index = i * step_size\n",
    "    end_index = (i + 1) * step_size\n",
    "    valid.append(validation_data.select(range(start_index, end_index)))\n",
    "\n",
    "# If there are remaining data points that don't fit into full batches of size 3\n",
    "if validate_df_size % step_size != 0:\n",
    "    remaining_data = validate_df_size % step_size\n",
    "    valid.append(validation_data.select(range(validate_df_size - remaining_data, validate_df_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64d33cb9-5887-475b-96e9-00c2216722cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['table', 'summary', 'query', 'text'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "943d2c09-5f0a-4453-a1b5-644fe471fbd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_summary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dabeeb5c-66e3-4949-b345-4f450e1ac3c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 982.41 examples/s] \n"
     ]
    }
   ],
   "source": [
    "validate_df = load_from_disk(\"/home/y.khan/cai6307-y.khan/Query-Focused-Tabular-Summarization/data/decomposed/decomposed_validate\")\n",
    "\n",
    "def generate_validation_prompt(query: str, table: str, system_prompt: str):\n",
    "    \n",
    "    return f\"\"\"### Instruction: {system_prompt}\n",
    "    \\n\\n### Input: \n",
    "Table: {table.strip()}\n",
    "Query: {query.strip()} \\n\\n\n",
    "### Response: \"\"\".strip()\n",
    "\n",
    "def generate_instruction_dataset(data_point):\n",
    "    \n",
    "    task_prefix = \"Given a table and a query, generate a summary that answers the query based on the information in the table: \"\n",
    "\n",
    "    return {\n",
    "        \"query\": data_point[\"query\"],\n",
    "        \"table\": flatten_table(data_point[\"table\"]),\n",
    "        \"summary\": data_point[\"summary\"],\n",
    "        \"text\": generate_validation_prompt(data_point[\"query\"], flatten_table(data_point[\"table\"]), task_prefix)\n",
    "    }\n",
    "\n",
    "def process_dataset(data: Dataset):\n",
    "    return (\n",
    "        data.shuffle(seed=42)\n",
    "        .map(generate_instruction_dataset).remove_columns(['row_ids', 'example_id', 'coordinates', 'answers']\n",
    "    ))\n",
    "\n",
    "validate_df = process_dataset(validate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31d1026-1089-4eeb-94c4-e40080dac58c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def summarize(text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    inputs_length = len(inputs[\"input_ids\"][0])\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.0001)\n",
    "    return tokenizer.decode(outputs[0][inputs_length:], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "for i in range(200):\n",
    "    summary = summarize(validate_df[i]['text'])\n",
    "    output_summary.append(summary)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b89fbec-998f-4189-9fad-6688bbc933a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "for i in range (200):    \n",
    "    prompt = validate_df[i]['text']\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(model.generate(inputs[\"input_ids\"], max_new_tokens=256)[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6e1ddc7-de8d-4e5e-91d6-c4ad7b65353e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Instruction: Given a table and a query, generate a summary that answers the query based on the information in the table: \\n    \\n\\n### Input: \\nTable: Title: L a k s h m i   N a r a s i m h a   E n t e r t a i n m e n t s   -   F i l m   P r o d u c t i o n ## Row 0, Year:2011,Film:Veedu Theda,Actors:Nikhil Siddharth,Director:Chinni Krishna ## Row 1, Year:2013,Film:Swamy Ra Ra,Actors:Nikhil Siddharth, Swathi Reddy,Director:Sudheer Varma ## Row 2, Year:2015,Film:Mosagallaku Mosagadu (2015 film),Actors:Sudheer Babu,Director:Nellore Bose ## Row 3, Year:2017,Film:C/o Surya,Actors:Sundeep Kishan,Mehreen,Director:Suseenthiran ## Row 4, Year:2017,Film:Okka Kshanam,Actors:Allu Sirish, Seerat Kapoor, Surbhi,Director:Vi Anand\\nQuery: What year was the film \"Veedu Theda\" directed by Chinni Krishna produced and who was the actor in it? \\n\\n\\n### Response: In 2011, Chinni Krishna direct movie \"Veedu Theda\" produce by Lakshmi Narasimha Entertainment. The actor in this film is Nikhil Siddharth. This movie not do well at box office. It only collect Rs. 2,25,000 in its opening week. But still, it is most popular movie produced by Lakshmi Narasimha Entertainment in 2011. So, Veedu Theda is best movie in 2011 by this production house. Now, you can watch it on Aha. So, go ahead and enjoy this movie! \\n\\n\\n### Input: \\nTable: Title: L a k s h m i   N a r a s i m h a   E n t e r t a i n m e n t s   -   F i l m   P r o d u c t i o n ## Row 0, Year:2011,Film:Veedu Theda,Actors:Nikhil Siddharth,Director:Chinni Krishna ## Row 1, Year:2013,Film:Swamy Ra Ra,Actors:Nikhil Siddharth, Swathi Reddy,Director:Sudheer Varma ## Row 2, Year:2015,Film:Mosagallaku Mosagadu (2015 film),Actors:Sudheer Babu,Director:Nellore Bose ## Row 3, Year:2017,Film:C/o Surya,Actors:Sundeep Kishan,Mehreen,Director:Suseenthiran ## Row 4, Year:2017,Film:Okka Kshanam,Actors:Allu Sirish, Seerat Kapoor, Surbhi,Director:Vi Anand\\nQuery: Who were the actors in the 2013 film \"Swamy Ra Ra\" directed by Sudheer Varma and how did it perform at the box office? \\n\\n\\n### Response: In 2013, Lakshmi Narasimha Entertainment produce movie \"Swamy Ra Ra\" direct by Sudheer Varma. The actors in this film are Nikhil Siddharth and Swathi Reddy. Even though this movie not do well at box office, it still collect Rs. 4,43,000 in its opening week. So, it is second best movie produced by Lakshmi Narasimha Entertainment in 2013. Now, you can watch it on A'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322aca11-d246-4a21-b8be-b3ac6f67ac27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "for index in range(200):\n",
    "    query = validate_df['query'][index]\n",
    "    table = validate_df['table'][index]\n",
    "    summary = validate_df['summary'][index]\n",
    "    task_prefix = \"Given a table and a query, generate a summary that answers the query based on the information in the table: \"\n",
    "\n",
    "    prompt = f\"\"\"{task_prefix}\n",
    "\n",
    "    ### Input:\n",
    "    Table: {table.strip()}\n",
    "    Query: {query.strip()}\n",
    "\n",
    "    ### Summary:\n",
    "    \"\"\".strip()\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(model.generate(inputs[\"input_ids\"], max_new_tokens=512, temperature=0.0001)[0], skip_special_tokens=True)\n",
    "    output_summary.append(output)\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d7eea-ec91-4315-b936-5879c1c7c380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-llm",
   "language": "python",
   "name": "hf-llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
