{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d0d62a-963b-4df9-9278-391052afc7d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "import torch \n",
    "warnings.filterwarnings('ignore')\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b4ccc95-bcf2-4757-952e-184e5181f768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from typing import List, Dict\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    ")\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import evaluate\n",
    "from typing import List, Dict\n",
    "\n",
    "train_df = load_from_disk(\"/home/y.khan/cai6307-y.khan/Query-Focused-Tabular-Summarization/data/decomposed/decomposed_train\")\n",
    "test_df = load_from_disk(\"/home/y.khan/cai6307-y.khan/Query-Focused-Tabular-Summarization/data/decomposed/decomposed_test\")\n",
    "validate_df = load_from_disk(\"/home/y.khan/cai6307-y.khan/Query-Focused-Tabular-Summarization/data/decomposed/decomposed_validate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f271ea66-73c0-4194-8af7-1baada76f43d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 500/500 [00:02<00:00, 181.68 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['row_ids', 'table', 'summary', 'query', 'example_id', 'coordinates', 'answers'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example_ids = set(test_df['example_id'])\n",
    "validate_example_ids = set(validate_df['example_id'])\n",
    "common_example_ids = test_example_ids.intersection(validate_example_ids)\n",
    "\n",
    "test_df = test_df.filter(lambda example: example['example_id'] not in common_example_ids)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9170302f-7e4e-4ee1-8e18-00e0881cee85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten_table(table: Dict) -> str:\n",
    "    header = table.get('header', [])\n",
    "    rows = table.get('rows', [])\n",
    "    \n",
    "    flattened_rows = []\n",
    "    for i, row in enumerate(rows):\n",
    "        row_text = f\"Row {i}, \" + \",\".join([f\"{col}:{val}\" for col, val in zip(header, row)])\n",
    "        flattened_rows.append(\"## \" + row_text)\n",
    "\n",
    "    flattened_table = \" \".join(flattened_rows)\n",
    "    return flattened_table\n",
    "\n",
    "def generate_validate_prompt(examples):\n",
    "    table = examples['table']\n",
    "    query = examples['query']\n",
    "    summary = examples['summary']\n",
    "    table_title = table['title']\n",
    "    system_prompt = \"You are a helpful, respectful and honest assistant. Below is an instruction that describes a query-focused summarization task. Write a summary that appropriately response to the user query.\"\n",
    "    \n",
    "    task = \"Using the information from the table, generate a paragraph-long summary to response to the following user query:\"\n",
    "\n",
    "    \n",
    "    flattened_table = flatten_table(table)\n",
    "    input_text = f\"Table Title: {table_title}\\n{flattened_table}\\n{task}\\nQuery: {query}\\n\\nSummary:\\n\"\n",
    "    prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "{input_text} [/INST]\"\"\"\n",
    "    prompt = f\"{system_prompt}\\n{input_text}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23c7c872-deb0-45e2-9588-d3073b77c0ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful, respectful and honest assistant. Below is an instruction that describes a query-focused summarization task. Write a summary that appropriately response to the user query.\n",
      "Table Title: Swiss Locomotive And Machine Works\n",
      "## Row 0, Built:1895,Number:1,Type:Mountain Railway Rack Steam Locomotive,Slm Number:923,Wheel Arrangement:0 - 4 - 2 T,Location:Snowdon Mountain Railway ## Row 1, Built:1895,Number:2,Type:Mountain Railway Rack Steam Locomotive,Slm Number:924,Wheel Arrangement:0 - 4 - 2 T,Location:Snowdon Mountain Railway ## Row 2, Built:1895,Number:3,Type:Mountain Railway Rack Steam Locomotive,Slm Number:925,Wheel Arrangement:0 - 4 - 2 T,Location:Snowdon Mountain Railway ## Row 3, Built:1896,Number:4,Type:Mountain Railway Rack Steam Locomotive,Slm Number:988,Wheel Arrangement:0 - 4 - 2 T,Location:Snowdon Mountain Railway ## Row 4, Built:1896,Number:5,Type:Mountain Railway Rack Steam Locomotive,Slm Number:989,Wheel Arrangement:0 - 4 - 2 T,Location:Snowdon Mountain Railway ## Row 5, Built:1922,Number:6,Type:Mountain Railway Rack Steam Locomotive,Slm Number:2838,Wheel Arrangement:0 - 4 - 2 T,Location:Snowdon Mountain Railway ## Row 6, Built:1923,Number:7,Type:Mountain Railway Rack Steam Locomotive,Slm Number:2869,Wheel Arrangement:0 - 4 - 2 T,Location:Snowdon Mountain Railway ## Row 7, Built:1923,Number:8,Type:Mountain Railway Rack Steam Locomotive,Slm Number:2870,Wheel Arrangement:0 - 4 - 2 T,Location:Snowdon Mountain Railway\n",
      "Using the information from the table, generate a paragraph-long summary to response to the following user query:\n",
      "Query: Summarize the basic information of the locomotive(s) built by Swiss Locomotive and Machine Works with slm number 988.\n",
      "\n",
      "Summary:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = generate_validate_prompt(validate_df[1])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9af6bbc-9da3-47e5-a9cd-154d320808a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 15/15 [05:40<00:00, 22.67s/it]\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"daryl149/llama-2-70b-chat-hf\"\n",
    "cache_dir='./llama2-70B_cache'\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_dir,\n",
    "                                        token=\"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\",\n",
    "                                         quantization_config=nf4_config,\n",
    "                                        device_map=\"auto\",\n",
    "                                        cache_dir=cache_dir\n",
    "                                        )\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_dir, \n",
    "                                           token=\"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\",\n",
    "                                           trust_remote_code=True, \n",
    "                                           cache_dir=cache_dir\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38fb50d3-97db-4ed3-ae03-650f7f83ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b22b354-c3ab-439f-83af-191c38a7419f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_summary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e2eb7ea-33c0-45b1-a6f1-987e52f1fe45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizer class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      " 66%|██████▌   | 132/200 [1:09:28<31:47, 28.05s/it]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "100%|██████████| 200/200 [1:46:38<00:00, 31.99s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(200)):\n",
    "    prompt = generate_validate_prompt(validate_df[i])\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful, respectful and honest assistant. Below is an instruction that describes a query-focused summarization task. Write a summary that appropriately response to the user query.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=400,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.0001,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    output_summary.append(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29f7b0ba-765b-4746-bab1-d9ef7d748d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten_table(table: Dict) -> str:\n",
    "    header = table.get('header', [])\n",
    "    rows = table.get('rows', [])\n",
    "    \n",
    "    flattened_rows = []\n",
    "    for i, row in enumerate(rows):\n",
    "        row_text = f\"Row {i}, \" + \",\".join([f\"{col}:{val}\" for col, val in zip(header, row)])\n",
    "        flattened_rows.append(\"## \" + row_text)\n",
    "\n",
    "    flattened_table = \" \".join(flattened_rows)\n",
    "    return flattened_table\n",
    "\n",
    "def generate_validate_prompt(examples):\n",
    "    table = examples['table']\n",
    "    query = examples['query']\n",
    "    summary = examples['summary']\n",
    "    table_title = table['title']\n",
    "    system_prompt = \"You are a helpful, respectful and honest assistant. Below is an instruction that describes a query-focused summarization task. Write a summary that appropriately response to the user query.\"\n",
    "    \n",
    "    task = \"Using the information from the table, generate a paragraph-long summary to response to the following user query:\"\n",
    "\n",
    "    \n",
    "    flattened_table = flatten_table(table)\n",
    "    input_text = f\"Table Title: {table_title}\\n{flattened_table}\\n{task}\\nQuery: {query}\\n\\nSummary:\\n\"\n",
    "    prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "{input_text} [/INST]\"\"\"\n",
    "    #prompt = f\"{system_prompt}\\n{input_text}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09b0495c-641a-413c-a003-841e6e8e1ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_summary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8699fb05-69f1-42a3-b739-3362e647de37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Below is an instruction that describes a query-focused summarization task. Write a summary that appropriately response to the user query.\n",
      "<</SYS>>\n",
      "Table Title: List of colleges and universities in Maine - Open institutions\n",
      "## Row 0, Control:Private,Type:Baccalaureate college ## Row 1, Control:Private (for-profit),Type:Associates college ## Row 2, Control:Private,Type:Baccalaureate college ## Row 3, Control:Public,Type:Associates college ## Row 4, Control:Private,Type:Baccalaureate college ## Row 5, Control:Private,Type:Baccalaureate college ## Row 6, Control:Public,Type:Associates college ## Row 7, Control:Private,Type:Masters university ## Row 8, Control:Private,Type:Baccalaureate college ## Row 9, Control:Public,Type:Associates college ## Row 10, Control:Private,Type:School of art ## Row 11, Control:Public,Type:Baccalaureate college ## Row 12, Control:Private,Type:Associates college ## Row 13, Control:Public,Type:Associates college ## Row 14, Control:Private,Type:Masters university ## Row 15, Control:Public,Type:Associates college ## Row 16, Control:Private,Type:Baccalaureate college ## Row 17, Control:Private,Type:Baccalaureate college ## Row 18, Control:Public,Type:Research university ## Row 19, Control:Public,Type:Baccalaureate college ## Row 20, Control:Public,Type:Baccalaureate college ## Row 21, Control:Public,Type:Baccalaureate college ## Row 22, Control:Public,Type:Baccalaureate college ## Row 23, Control:Public,Type:Baccalaureate college ## Row 24, Control:Private,Type:Masters university ## Row 25, Control:Public,Type:Masters university ## Row 26, Control:Public,Type:Associates college ## Row 27, Control:Public,Type:Associates college\n",
      "Using the information from the table, generate a paragraph-long summary to response to the following user query:\n",
      "Query: How many private versus public institutions are there and which category has a greater total enrollment in 2016?\n",
      "\n",
      "Summary:\n",
      " [/INST]\n"
     ]
    }
   ],
   "source": [
    "print(generate_validate_prompt(validate_df[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e2500a1-4047-4150-99f4-ef75b4dee471",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [1:36:55<00:00, 29.08s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(200)):\n",
    "    prompt = generate_validate_prompt(validate_df[i])\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful, respectful and honest assistant. Below is an instruction that describes a query-focused summarization task. Write a summary that appropriately response to the user query.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    # prompt = pipeline.tokenizer.apply_chat_template(\n",
    "    #         messages, \n",
    "    #         tokenize=False, \n",
    "    #         add_generation_prompt=True\n",
    "    # )\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=400,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.0001,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    final_summary.append(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f83d5743-69a1-4d2e-8f73-15a4b33d545d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.43802416271277406, 'rouge2': 0.22315055717637833, 'rougeL': 0.3339567177992043, 'rougeLsum': 0.3351979311861989} {'precision': [0.9254542589187622, 0.903687059879303, 0.892741858959198, 0.8382596969604492, 0.8795133233070374, 0.86527419090271, 0.8924967050552368, 0.9122763872146606, 0.873224139213562, 0.8648399114608765, 0.8779414892196655, 0.8244060277938843, 0.8873380422592163, 0.890691876411438, 0.8413833975791931, 0.9511193037033081, 0.8795099854469299, 0.8976823091506958, 0.8666635751724243, 0.904883861541748, 0.8972246646881104, 0.8855209350585938, 0.8773843050003052, 0.9489731192588806, 0.8841674327850342, 0.9214867353439331, 0.8996047973632812, 0.9254379272460938, 0.9023730754852295, 0.8697454929351807, 0.8328706622123718, 0.8654690980911255, 0.8901695013046265, 0.8708077073097229, 0.8705869317054749, 0.8608335256576538, 0.9153760671615601, 0.9522333741188049, 0.8936750888824463, 0.8720929622650146, 0.9089547395706177, 0.8584100008010864, 0.8707201480865479, 0.924479067325592, 0.9387384653091431, 0.8806690573692322, 0.9211636781692505, 0.8975293636322021, 0.8593438863754272, 0.8890705704689026, 0.8782400488853455, 0.9327939748764038, 0.9057838916778564, 0.9012045860290527, 0.8783629536628723, 0.8722383975982666, 0.8670471906661987, 0.8239976167678833, 0.8434712290763855, 0.8785351514816284, 0.8444464206695557, 0.8885930776596069, 0.8645700216293335, 0.8342170715332031, 0.9162824749946594, 0.8963104486465454, 0.8622924089431763, 0.8526854515075684, 0.8534443974494934, 0.8400183320045471, 0.8620424270629883, 0.8962578773498535, 0.9157181978225708, 0.9345921277999878, 0.9435945153236389, 0.8364866971969604, 0.883739173412323, 0.8882431387901306, 0.8708231449127197, 0.8991201519966125, 0.8972192406654358, 0.953280508518219, 0.8472570776939392, 0.855469286441803, 0.9353266954421997, 0.9338135719299316, 0.9333671927452087, 0.8631634712219238, 0.8659154176712036, 0.8784887790679932, 0.9538195729255676, 0.8960639834403992, 0.94157475233078, 0.9028086066246033, 0.8919682502746582, 0.8720791339874268, 0.8769575953483582, 0.8635895252227783, 0.9419105052947998, 0.8765581250190735, 0.8760972023010254, 0.8470734357833862, 0.8532552719116211, 0.9040412902832031, 0.895168662071228, 0.8715335130691528, 0.8896490335464478, 0.9532124996185303, 0.8909019231796265, 0.8713533282279968, 0.9533650875091553, 0.8637813329696655, 0.8952326774597168, 0.8853726983070374, 0.928918182849884, 0.8248394727706909, 0.8835771083831787, 0.9427851438522339, 0.8905373811721802, 0.8602798581123352, 0.8497152328491211, 0.9417520761489868, 0.8861261606216431, 0.8174720406532288, 0.8403711915016174, 0.9436686038970947, 0.9110028743743896, 0.8596859574317932, 0.8327837586402893, 0.8617650270462036, 0.892673134803772, 0.8667705655097961, 0.8833654522895813, 0.8940335512161255, 0.8746187090873718, 0.8597491979598999, 0.8572048544883728, 0.891162633895874, 0.8808478116989136, 0.8103318214416504, 0.9002892971038818, 0.8780653476715088, 0.9288707375526428, 0.888392448425293, 0.9044225215911865, 0.8710683584213257, 0.8924331665039062, 0.9202658534049988, 0.8440347909927368, 0.8640760183334351, 0.9105774164199829, 0.9133480191230774, 0.8775671124458313, 0.8697896003723145, 0.8636763095855713, 0.8687558770179749, 0.8947678208351135, 0.8932653665542603, 0.8218718767166138, 0.8570660352706909, 0.8508826494216919, 0.866089940071106, 0.8780720829963684, 0.9220442175865173, 0.9641895890235901, 0.932087779045105, 0.9574571847915649, 0.950309157371521, 0.9145479798316956, 0.9174556732177734, 0.9390107989311218, 0.8708310127258301, 0.9320379495620728, 0.8860343098640442, 0.9018162488937378, 0.8761414289474487, 0.8976173400878906, 0.8549058437347412, 0.8989913463592529, 0.891081690788269, 0.8946107029914856, 0.8684077262878418, 0.9158247113227844, 0.8974893093109131, 0.8885124921798706, 0.8614661693572998, 0.8914231061935425, 0.9208427667617798, 0.8844416737556458, 0.8858644962310791, 0.9486339092254639, 0.8492557406425476, 0.8647385239601135, 0.9265084862709045, 0.9636825919151306, 0.9170277118682861, 0.9357925057411194, 0.8560183048248291, 0.9060056805610657, 0.8605360984802246], 'recall': [0.884797990322113, 0.9407227039337158, 0.8798177242279053, 0.8024653196334839, 0.8954439759254456, 0.9327809810638428, 0.8490808010101318, 0.9218577146530151, 0.8717846870422363, 0.8962986469268799, 0.9095364809036255, 0.9176073670387268, 0.86606365442276, 0.872913658618927, 0.9160840511322021, 0.966166615486145, 0.8784075975418091, 0.862318217754364, 0.8862345218658447, 0.929196834564209, 0.9283554553985596, 0.9350187182426453, 0.8720923662185669, 0.9420335292816162, 0.878334641456604, 0.9108163118362427, 0.9196405410766602, 0.8702410459518433, 0.9256168603897095, 0.8706032037734985, 0.8240517377853394, 0.8763055801391602, 0.8875080943107605, 0.8911077976226807, 0.9120382070541382, 0.8955325484275818, 0.9212828278541565, 0.9164654016494751, 0.8865534663200378, 0.8874702453613281, 0.9351842999458313, 0.9283940196037292, 0.8606295585632324, 0.9347777962684631, 0.9602919816970825, 0.8720030784606934, 0.910937488079071, 0.8953027725219727, 0.8795692920684814, 0.9354175329208374, 0.8986396789550781, 0.9263580441474915, 0.8961472511291504, 0.900768518447876, 0.9154043197631836, 0.9065542221069336, 0.8635936975479126, 0.8829962611198425, 0.8571283221244812, 0.9241751432418823, 0.8706647157669067, 0.9269910454750061, 0.8548972010612488, 0.8567055463790894, 0.9282292723655701, 0.9344881176948547, 0.896977961063385, 0.8822813034057617, 0.8926303386688232, 0.8776544332504272, 0.8730393648147583, 0.8730109930038452, 0.927427351474762, 0.918588399887085, 0.9364423751831055, 0.8672605752944946, 0.9307342171669006, 0.8874610066413879, 0.9012713432312012, 0.974346399307251, 0.9621747136116028, 0.9354233145713806, 0.8888523578643799, 0.9522936344146729, 0.9206494092941284, 0.9355618357658386, 0.9459976553916931, 0.8945151567459106, 0.8942683935165405, 0.893498420715332, 0.9436900615692139, 0.9212460517883301, 0.8775908946990967, 0.8862167000770569, 0.9067273139953613, 0.8652759790420532, 0.8862969279289246, 0.9055493474006653, 0.8954207897186279, 0.8922564387321472, 0.9241656064987183, 0.8751126527786255, 0.902219295501709, 0.8817770481109619, 0.9141666889190674, 0.9233092665672302, 0.9123344421386719, 0.9652528166770935, 0.9385751485824585, 0.8903815150260925, 0.9072010517120361, 0.898209810256958, 0.9361487030982971, 0.8580223917961121, 0.9266349077224731, 0.8919800519943237, 0.897921621799469, 0.9273614883422852, 0.9196164608001709, 0.8851814270019531, 0.874839723110199, 0.9550117254257202, 0.9154601693153381, 0.8802326917648315, 0.9064056873321533, 0.9089946746826172, 0.9104506969451904, 0.9118738174438477, 0.8704825639724731, 0.9008178114891052, 0.9460636377334595, 0.9019612073898315, 0.8826741576194763, 0.8958933353424072, 0.8582651615142822, 0.8951871991157532, 0.868500292301178, 0.9002189636230469, 0.9226018786430359, 0.8883339166641235, 0.9033699631690979, 0.8855287432670593, 0.9127481579780579, 0.8698371648788452, 0.909691333770752, 0.8787282109260559, 0.9079106450080872, 0.9019604921340942, 0.8677793741226196, 0.8861404657363892, 0.8929957747459412, 0.8734052777290344, 0.926461398601532, 0.8686559200286865, 0.8594023585319519, 0.8689240217208862, 0.8983096480369568, 0.9300613403320312, 0.8505945205688477, 0.9303624629974365, 0.8687927722930908, 0.9217509031295776, 0.9171372652053833, 0.9445828199386597, 0.9679807424545288, 0.9454352855682373, 0.9613469839096069, 0.9244092702865601, 0.9183663129806519, 0.9280198812484741, 0.9669995307922363, 0.899716854095459, 0.8933005332946777, 0.9071851968765259, 0.9225387573242188, 0.8768081665039062, 0.8733927607536316, 0.8683281540870667, 0.8960683345794678, 0.9365904331207275, 0.9145535230636597, 0.875827431678772, 0.8907896280288696, 0.9294991493225098, 0.9119817018508911, 0.8823192715644836, 0.8345134258270264, 0.9227699041366577, 0.8736770153045654, 0.9323040843009949, 0.9471883773803711, 0.9184650182723999, 0.9476550221443176, 0.901874303817749, 0.9480147957801819, 0.8944029211997986, 0.9457584023475647, 0.8865313529968262, 0.9178062677383423, 0.8638278245925903], 'f1': [0.9046696424484253, 0.9218330979347229, 0.8862326145172119, 0.8199720978736877, 0.8874071836471558, 0.8977603316307068, 0.8702476024627686, 0.917042076587677, 0.8725038170814514, 0.880288302898407, 0.8934597373008728, 0.8685134649276733, 0.8765717148780823, 0.8817131519317627, 0.8771461844444275, 0.9585838913917542, 0.8789584040641785, 0.8796449899673462, 0.8763397932052612, 0.9168792366981506, 0.9125246405601501, 0.9095969200134277, 0.8747303485870361, 0.9454905390739441, 0.8812413811683655, 0.9161204695701599, 0.9095123410224915, 0.8969911336898804, 0.9138472080230713, 0.8701741695404053, 0.8284378051757812, 0.8708536028862, 0.8888368010520935, 0.8808408379554749, 0.8908306360244751, 0.8778402805328369, 0.9183199405670166, 0.9340070486068726, 0.8901000022888184, 0.8797143697738647, 0.9218829274177551, 0.8920314908027649, 0.8656454086303711, 0.929599940776825, 0.9493929147720337, 0.8763145804405212, 0.9160220623016357, 0.8964146971702576, 0.8693389892578125, 0.9116553664207458, 0.8883227109909058, 0.929564893245697, 0.9009397625923157, 0.9009864926338196, 0.8965011835098267, 0.8890653252601624, 0.8653169870376587, 0.8524773716926575, 0.85024493932724, 0.9007773995399475, 0.8573551774024963, 0.9073860049247742, 0.859706461429596, 0.8453117609024048, 0.9222171902656555, 0.9150012135505676, 0.8792932033538818, 0.8672309517860413, 0.8725976943969727, 0.8584240078926086, 0.8675060272216797, 0.8844817280769348, 0.9215355515480042, 0.9265211820602417, 0.940004825592041, 0.8515956997871399, 0.9066280722618103, 0.887851893901825, 0.885785698890686, 0.9352229833602905, 0.9285624027252197, 0.9442675113677979, 0.8675563931465149, 0.9012885093688965, 0.9279299974441528, 0.9346868991851807, 0.9396399855613708, 0.8785597085952759, 0.8798635601997375, 0.8859300017356873, 0.9487278461456299, 0.9084805250167847, 0.9084575772285461, 0.894435703754425, 0.899287223815918, 0.8686642646789551, 0.8816025257110596, 0.8840718865394592, 0.9180775284767151, 0.8843376040458679, 0.8994896411895752, 0.8608647584915161, 0.8770544528961182, 0.8927703499794006, 0.9045679569244385, 0.896674633026123, 0.900848925113678, 0.9591948390007019, 0.9141173958778381, 0.8807646632194519, 0.929710328578949, 0.8806592226028442, 0.9152336716651917, 0.8714830279350281, 0.9277751445770264, 0.8570969104766846, 0.8906916379928589, 0.9350097179412842, 0.9048433899879456, 0.8725530505180359, 0.8620944023132324, 0.9483355283737183, 0.9005542993545532, 0.8476923108100891, 0.8721402883529663, 0.9260072112083435, 0.9107266664505005, 0.8850111961364746, 0.8512158989906311, 0.8808587789535522, 0.9185932278633118, 0.8840157985687256, 0.8830196857452393, 0.89496248960495, 0.8663647174835205, 0.8771103620529175, 0.8628156185150146, 0.8956679105758667, 0.9012415409088135, 0.8475419282913208, 0.9018270373344421, 0.881781280040741, 0.9207388758659363, 0.8790168762207031, 0.907049298286438, 0.8748815059661865, 0.9001054167747498, 0.9110212326049805, 0.8557423949241638, 0.8749691247940063, 0.9017009139060974, 0.8929302096366882, 0.9013516306877136, 0.8692224025726318, 0.8615339994430542, 0.8688399195671082, 0.8965352773666382, 0.9112920761108398, 0.8359865546226501, 0.8922114372253418, 0.8597444295883179, 0.8930539488792419, 0.8971796035766602, 0.9331774711608887, 0.9660813808441162, 0.9387140870094299, 0.9593981504440308, 0.9371802806854248, 0.9164531230926514, 0.9227075576782227, 0.9527996182441711, 0.8850383162498474, 0.9122582077980042, 0.8964849710464478, 0.9120597839355469, 0.8764746785163879, 0.8853393793106079, 0.8615647554397583, 0.8975274562835693, 0.9132694602012634, 0.9044721722602844, 0.8721017837524414, 0.9031336903572083, 0.9132137894630432, 0.9000941514968872, 0.871768057346344, 0.8620299696922302, 0.9218053817749023, 0.8790264129638672, 0.9084911346435547, 0.9479106068611145, 0.8825055956840515, 0.9043000936508179, 0.9140254259109497, 0.9557844996452332, 0.9055740237236023, 0.9407490491867065, 0.8710076808929443, 0.9118677377700806, 0.8621788024902344], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.36.2)'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "rougeL = []\n",
    "bert = []\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "rougescore = evaluate.load(\"rouge\")\n",
    "\n",
    "bert_score = bertscore.compute(predictions=output_summary, references=validate_df['summary'], lang = \"en\")\n",
    "rouge_score = rougescore.compute(predictions=output_summary, references=validate_df['summary'])\n",
    "print(rouge_score, bert_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1828c039-5b53-4e52-8c9d-f14b2b891eb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.444762031715928, 'rouge2': 0.22516545970994076, 'rougeL': 0.33784133302841585, 'rougeLsum': 0.3411821198615538} {'precision': [0.9053110480308533, 0.8945354223251343, 0.8954221606254578, 0.8558622598648071, 0.8250436186790466, 0.8652742505073547, 0.9143372178077698, 0.8892030715942383, 0.8672518730163574, 0.8528574109077454, 0.8572314977645874, 0.9259178638458252, 0.869687020778656, 0.854345440864563, 0.8598247766494751, 0.8794569373130798, 0.883024275302887, 0.8759243488311768, 0.8770837783813477, 0.8936570882797241, 0.9294285774230957, 0.8988633751869202, 0.8814058899879456, 0.9195881485939026, 0.8782351613044739, 0.9261015057563782, 0.8982058763504028, 0.9001312255859375, 0.9263134002685547, 0.8639668226242065, 0.8327509760856628, 0.8694016933441162, 0.8950292468070984, 0.8727460503578186, 0.8306674361228943, 0.8697312474250793, 0.8640204668045044, 0.9067051410675049, 0.8907931447029114, 0.8812340497970581, 0.9065768718719482, 0.8747454285621643, 0.8807099461555481, 0.8831589221954346, 0.9387384653091431, 0.8918873071670532, 0.9066281318664551, 0.8954328298568726, 0.8620141744613647, 0.8668925762176514, 0.8706797361373901, 0.9448539614677429, 0.8952581286430359, 0.9456045031547546, 0.8582924008369446, 0.8488013744354248, 0.8766891956329346, 0.8511765599250793, 0.849720299243927, 0.833419680595398, 0.8620890378952026, 0.9250312447547913, 0.8462086319923401, 0.8683182001113892, 0.8807544112205505, 0.8894076347351074, 0.8845103979110718, 0.8634509444236755, 0.877160906791687, 0.8567558526992798, 0.8624768257141113, 0.907859206199646, 0.9274725317955017, 0.9174976348876953, 0.8741605877876282, 0.8468445539474487, 0.905584454536438, 0.8798333406448364, 0.867512047290802, 0.8991201519966125, 0.8798173666000366, 0.9320560693740845, 0.848718523979187, 0.8371936082839966, 0.8771556615829468, 0.9023259878158569, 0.9133564233779907, 0.8421741724014282, 0.8851873278617859, 0.8982751369476318, 0.952221691608429, 0.8973612785339355, 0.8709459900856018, 0.8406339287757874, 0.9056156873703003, 0.8791682720184326, 0.8630900382995605, 0.8826630115509033, 0.8874680399894714, 0.8849091529846191, 0.8952708840370178, 0.8895263671875, 0.8414993286132812, 0.9080464243888855, 0.9093490839004517, 0.8841060400009155, 0.9353159666061401, 0.935001790523529, 0.9007657766342163, 0.8534750938415527, 0.9523948431015015, 0.8776553273200989, 0.8957818150520325, 0.8767527341842651, 0.8837569952011108, 0.822891116142273, 0.8815428018569946, 0.9400492310523987, 0.8936977386474609, 0.8757277727127075, 0.8786383867263794, 0.9495523571968079, 0.8458229303359985, 0.8288282155990601, 0.8408843874931335, 0.9439874887466431, 0.9103777408599854, 0.8545904159545898, 0.8452736735343933, 0.8158643245697021, 0.8974388241767883, 0.9209777116775513, 0.8838324546813965, 0.8940335512161255, 0.8994219303131104, 0.8548795580863953, 0.8711450099945068, 0.8977693319320679, 0.8712998032569885, 0.828749418258667, 0.8955936431884766, 0.8562338948249817, 0.8932027220726013, 0.8892604112625122, 0.9063214063644409, 0.8464155197143555, 0.8902555704116821, 0.9202658534049988, 0.8606483936309814, 0.8583066463470459, 0.8985458612442017, 0.940168023109436, 0.8927426934242249, 0.8485277891159058, 0.8624464273452759, 0.8864682912826538, 0.8958481550216675, 0.8833935856819153, 0.8310694694519043, 0.9599173665046692, 0.8546517491340637, 0.8707542419433594, 0.8693740963935852, 0.9232405424118042, 0.9113207459449768, 0.9113122820854187, 0.9266295433044434, 0.8804553747177124, 0.9089659452438354, 0.9174556136131287, 0.939010739326477, 0.8244518041610718, 0.9437639117240906, 0.8950267434120178, 0.8613368272781372, 0.8673262596130371, 0.8986671566963196, 0.8562179803848267, 0.8790196776390076, 0.9237271547317505, 0.9102210998535156, 0.901443600654602, 0.9157879948616028, 0.901722252368927, 0.8738782405853271, 0.8742725849151611, 0.8468232154846191, 0.8826113939285278, 0.8906922340393066, 0.9175298810005188, 0.9506909847259521, 0.8608005046844482, 0.8893553018569946, 0.8908517956733704, 0.9657371640205383, 0.8758533000946045, 0.8861789107322693, 0.8781239986419678, 0.9020642042160034, 0.8543802499771118], 'recall': [0.8867178559303284, 0.9342986345291138, 0.8805902600288391, 0.8197281360626221, 0.9134187698364258, 0.9327809810638428, 0.8477274179458618, 0.9122847318649292, 0.8880040645599365, 0.904525637626648, 0.9021484851837158, 0.9367098808288574, 0.8542768955230713, 0.8727975487709045, 0.9110077619552612, 0.9560564756393433, 0.8816772699356079, 0.8478718996047974, 0.8878097534179688, 0.9254317879676819, 0.9424658417701721, 0.9403857588768005, 0.8701900243759155, 0.9360452890396118, 0.8800609707832336, 0.911861777305603, 0.9219467043876648, 0.8761965036392212, 0.9221742749214172, 0.8666436076164246, 0.8219485282897949, 0.8696238398551941, 0.8893835544586182, 0.8871350884437561, 0.8900654315948486, 0.8945454955101013, 0.9168025255203247, 0.9139649868011475, 0.8803818225860596, 0.8938924670219421, 0.9431926012039185, 0.9388021230697632, 0.8714578151702881, 0.9230273962020874, 0.9602919816970825, 0.9143170118331909, 0.9040564298629761, 0.8954041004180908, 0.8769274950027466, 0.9105119705200195, 0.8956326842308044, 0.922778308391571, 0.9034647345542908, 0.898918628692627, 0.8989718556404114, 0.8977124691009521, 0.8699779510498047, 0.8958538770675659, 0.8622968792915344, 0.9157935976982117, 0.8604551553726196, 0.9402748942375183, 0.8419550657272339, 0.8601241111755371, 0.9278533458709717, 0.9360060691833496, 0.9073145389556885, 0.8711404800415039, 0.8970514535903931, 0.8859784603118896, 0.8479182124137878, 0.87804114818573, 0.9302821159362793, 0.91120445728302, 0.9231491088867188, 0.8641666173934937, 0.9308458566665649, 0.8827922344207764, 0.8978685736656189, 0.974346399307251, 0.9425069093704224, 0.9436062574386597, 0.8852976560592651, 0.9329493641853333, 0.9078289270401001, 0.9300505518913269, 0.901879072189331, 0.8745437860488892, 0.895110011100769, 0.9188927412033081, 0.9419064521789551, 0.9199776649475098, 0.8587173223495483, 0.8827630281448364, 0.8871274590492249, 0.8724172115325928, 0.8531509637832642, 0.9191898107528687, 0.8836295008659363, 0.8968644142150879, 0.9235443472862244, 0.9051425457000732, 0.9035593271255493, 0.8895404934883118, 0.9248001575469971, 0.919207751750946, 0.9148929119110107, 0.9442668557167053, 0.9340129494667053, 0.884351372718811, 0.9054626226425171, 0.872921347618103, 0.9484404921531677, 0.8787456154823303, 0.914532482624054, 0.8863498568534851, 0.8945561647415161, 0.9270004034042358, 0.9085829257965088, 0.8894526958465576, 0.8561925888061523, 0.9597787857055664, 0.8802096843719482, 0.8817472457885742, 0.9102051854133606, 0.9098792672157288, 0.9154369235038757, 0.8986515998840332, 0.8690378665924072, 0.8887065052986145, 0.9316637516021729, 0.9125902652740479, 0.886306881904602, 0.8958933353424072, 0.8786764144897461, 0.8875099420547485, 0.8661041259765625, 0.907573401927948, 0.9154249429702759, 0.8898793458938599, 0.9014360308647156, 0.8645076751708984, 0.9223989248275757, 0.8832079172134399, 0.9175573587417603, 0.8566558361053467, 0.9068244695663452, 0.9019604921340942, 0.8823235630989075, 0.8742737770080566, 0.9083911180496216, 0.8979300856590271, 0.9266692399978638, 0.8709398508071899, 0.862334668636322, 0.8755586743354797, 0.8961467742919922, 0.9188843369483948, 0.8498779535293579, 0.962982714176178, 0.8740110993385315, 0.8947890400886536, 0.91379714012146, 0.9458301663398743, 0.9440230131149292, 0.9088771343231201, 0.9279411435127258, 0.9001279473304749, 0.9146903157234192, 0.9280198812484741, 0.9669995307922363, 0.874155580997467, 0.8856634497642517, 0.9001038074493408, 0.8998233079910278, 0.8814675211906433, 0.8695693612098694, 0.8622744083404541, 0.8920038938522339, 0.9456166625022888, 0.9233497381210327, 0.8886833786964417, 0.8675908446311951, 0.9105638265609741, 0.9089065790176392, 0.8927770853042603, 0.8254427909851074, 0.9096620678901672, 0.8720788955688477, 0.9325366616249084, 0.9449591636657715, 0.9167641401290894, 0.9412333965301514, 0.8789049983024597, 0.9578052759170532, 0.8841836452484131, 0.9333515167236328, 0.8849294185638428, 0.9137570858001709, 0.8586135506629944], 'f1': [0.8959180116653442, 0.9139847755432129, 0.8879442811012268, 0.8374056220054626, 0.8669848442077637, 0.8977603912353516, 0.8797733187675476, 0.900596022605896, 0.8775053024291992, 0.8779319524765015, 0.8791166543960571, 0.931282639503479, 0.8619130849838257, 0.8634729385375977, 0.8846765756607056, 0.9161583781242371, 0.8823502659797668, 0.8616698980331421, 0.8824141621589661, 0.9092668890953064, 0.9359018206596375, 0.9191558957099915, 0.8757619857788086, 0.927743673324585, 0.8791471123695374, 0.918926477432251, 0.9099214673042297, 0.8880025744438171, 0.9242392778396606, 0.8653032183647156, 0.8273144960403442, 0.8695127367973328, 0.8921974897384644, 0.879881739616394, 0.8593412041664124, 0.8819639086723328, 0.8896292448043823, 0.9103205800056458, 0.8855569362640381, 0.8875181674957275, 0.924522340297699, 0.9056425094604492, 0.8760595321655273, 0.902653157711029, 0.9493929147720337, 0.902962863445282, 0.9053404927253723, 0.8954184651374817, 0.8694068789482117, 0.8881670236587524, 0.8829799890518188, 0.9336856603622437, 0.8993427157402039, 0.92167067527771, 0.8781612515449524, 0.8725720643997192, 0.8733206391334534, 0.8729439377784729, 0.855962336063385, 0.8726670742034912, 0.8612713813781738, 0.9325907826423645, 0.8440764546394348, 0.8642017245292664, 0.9036905765533447, 0.9121120572090149, 0.8957673907279968, 0.8672786355018616, 0.8869946599006653, 0.8711221814155579, 0.8551355600357056, 0.892701268196106, 0.9288751482963562, 0.9143401980400085, 0.8979872465133667, 0.8554179072380066, 0.9180414080619812, 0.8813102841377258, 0.8824293613433838, 0.9352229833602905, 0.9100838303565979, 0.9377956390380859, 0.866622269153595, 0.8824815154075623, 0.8922287225723267, 0.9159785509109497, 0.9075814485549927, 0.8580538034439087, 0.8901210427284241, 0.9084669947624207, 0.9470360279083252, 0.908528745174408, 0.8647884130477905, 0.8611835241317749, 0.8962761759757996, 0.8757796883583069, 0.8580917716026306, 0.900556206703186, 0.8855445981025696, 0.8908466696739197, 0.9091878533363342, 0.8972665667533875, 0.8714258074760437, 0.8986982107162476, 0.9170095324516296, 0.9013153314590454, 0.9249917268753052, 0.9396114349365234, 0.9170880913734436, 0.8686389327049255, 0.9283359050750732, 0.8752819299697876, 0.9213593602180481, 0.8777480125427246, 0.8988813757896423, 0.8534424901008606, 0.8880017995834351, 0.9334792494773865, 0.9010788202285767, 0.8825368881225586, 0.867270290851593, 0.9546381831169128, 0.8626737594604492, 0.8544691801071167, 0.8741726875305176, 0.9266195893287659, 0.9129002690315247, 0.8760673403739929, 0.8569910526275635, 0.8507290482521057, 0.9142311215400696, 0.9167648553848267, 0.8850679397583008, 0.89496248960495, 0.8889281749725342, 0.8708891868591309, 0.8686172366142273, 0.9026447534561157, 0.892817497253418, 0.8582272529602051, 0.8985052704811096, 0.8603509664535522, 0.9075661301612854, 0.8862237930297852, 0.9119047522544861, 0.851504921913147, 0.8984636068344116, 0.9110212326049805, 0.8713512420654297, 0.8662166595458984, 0.9034416675567627, 0.9185637831687927, 0.9093896150588989, 0.8595877289772034, 0.8623905777931213, 0.8809796571731567, 0.8959974646568298, 0.9007894992828369, 0.8403685092926025, 0.9614475965499878, 0.8642230033874512, 0.8826081156730652, 0.891032338142395, 0.9343987703323364, 0.9273836612701416, 0.9100930690765381, 0.9272848963737488, 0.8901829719543457, 0.9118191003799438, 0.9227075576782227, 0.9527996182441711, 0.8485764265060425, 0.9137911200523376, 0.8975580930709839, 0.8801595568656921, 0.8743396997451782, 0.8838788270950317, 0.8592355251312256, 0.8854641914367676, 0.9345437288284302, 0.9167384505271912, 0.8950179815292358, 0.8910381197929382, 0.9061214327812195, 0.89104825258255, 0.8834279179573059, 0.8359963297843933, 0.8959325551986694, 0.8812872767448425, 0.924972414970398, 0.9478164315223694, 0.8879013657569885, 0.9145592451095581, 0.8848381042480469, 0.9617548584938049, 0.8799987435340881, 0.9091536998748779, 0.8815135359764099, 0.9078730344772339, 0.8564916253089905], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.36.2)'}\n"
     ]
    }
   ],
   "source": [
    "bert_score = bertscore.compute(predictions=final_summary, references=validate_df['summary'], lang = \"en\")\n",
    "rouge_score = rougescore.compute(predictions=final_summary, references=validate_df['summary'])\n",
    "print(rouge_score, bert_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a433c-26f3-4bf0-ba48-d19a6d28d5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-llm",
   "language": "python",
   "name": "hf-llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
