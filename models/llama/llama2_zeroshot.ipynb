{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4d0d62a-963b-4df9-9278-391052afc7d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "import torch \n",
    "warnings.filterwarnings('ignore')\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b4ccc95-bcf2-4757-952e-184e5181f768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from typing import List, Dict\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    ")\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import evaluate\n",
    "from typing import List, Dict\n",
    "\n",
    "train_df = load_from_disk(\"/home/y.khan/cai6307-y.khan/Query-Focused-Tabular-Summarization/data/decomposed/decomposed_train\")\n",
    "test_df = load_from_disk(\"/home/y.khan/cai6307-y.khan/Query-Focused-Tabular-Summarization/data/decomposed/decomposed_test\")\n",
    "validate_df = load_from_disk(\"/home/y.khan/cai6307-y.khan/Query-Focused-Tabular-Summarization/data/data/validate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f271ea66-73c0-4194-8af7-1baada76f43d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 500/500 [00:03<00:00, 146.79 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['row_ids', 'table', 'summary', 'query', 'example_id', 'coordinates', 'answers'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example_ids = set(test_df['example_id'])\n",
    "validate_example_ids = set(validate_df['example_id'])\n",
    "common_example_ids = test_example_ids.intersection(validate_example_ids)\n",
    "\n",
    "test_df = test_df.filter(lambda example: example['example_id'] not in common_example_ids)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9170302f-7e4e-4ee1-8e18-00e0881cee85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten_table(table: Dict) -> str:\n",
    "    header = table.get('header', [])\n",
    "    rows = table.get('rows', [])\n",
    "    \n",
    "    flattened_rows = []\n",
    "    for i, row in enumerate(rows):\n",
    "        row_text = f\"Row {i}, \" + \",\".join([f\"{col}:{val}\" for col, val in zip(header, row)])\n",
    "        flattened_rows.append(\"## \" + row_text)\n",
    "\n",
    "    flattened_table = \" \".join(flattened_rows)\n",
    "    return flattened_table\n",
    "\n",
    "def generate_validate_prompt(examples):\n",
    "    table = examples['table']\n",
    "    query = examples['query']\n",
    "    summary = examples['summary']\n",
    "    table_title = table['title']\n",
    "    system_prompt = \"You are a helpful, respectful and honest assistant. Below is an instruction that describes a query-focused summarization task. Write a summary that appropriately response to the user query.\"\n",
    "    \n",
    "    task = \"Using the information from the table, generate a paragraph-long summary to response to the following user query:\"\n",
    "\n",
    "    \n",
    "    flattened_table = flatten_table(table)\n",
    "    input_text = f\"Table Title: {table_title}\\n{flattened_table}\\n{task}\\nQuery: {query}\\n\\nSummary:\\n\"\n",
    "    prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "{input_text} [/INST]\"\"\"\n",
    "    prompt = f\"{system_prompt}\\n{input_text}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c7c872-deb0-45e2-9588-d3073b77c0ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful, respectful and honest assistant. Below is an instruction that describes a query-focused summarization task. Write a summary that appropriately response to the user query.\n",
      "Table Title: Swiss Locomotive And Machine Works\n",
      "## Row 0, Built:1895,Number:1,Type:Mountain Railway Rack Steam Locomotive,Slm Number:923,Wheel Arrangement:0 - 4 - 2 T,Location:Snowdon Mountain Railway,Notes:Ladas ## Row 1, Built:1895,Number:2,Type:Mountain Railway Rack Steam Locomotive,Slm Number:924,Wheel Arrangement:0 - 4 - 2 T,Location:Snowdon Mountain Railway,Notes:Enid ## Row 2, Built:1895,Number:3,Type:Mountain Railway Rack Steam Locomotive,Slm Number:925,Wheel Arrangement:0 - 4 - 2 T,Location:Snowdon Mountain Railway,Notes:Wyddfa ## Row 3, Built:1896,Number:4,Type:Mountain Railway Rack Steam Locomotive,Slm Number:988,Wheel Arrangement:0 - 4 - 2 T,Location:Snowdon Mountain Railway,Notes:Snowdon ## Row 4, Built:1896,Number:5,Type:Mountain Railway Rack Steam Locomotive,Slm Number:989,Wheel Arrangement:0 - 4 - 2 T,Location:Snowdon Mountain Railway,Notes:Moel Siabod ## Row 5, Built:1922,Number:6,Type:Mountain Railway Rack Steam Locomotive,Slm Number:2838,Wheel Arrangement:0 - 4 - 2 T,Location:Snowdon Mountain Railway,Notes:Padarn ## Row 6, Built:1923,Number:7,Type:Mountain Railway Rack Steam Locomotive,Slm Number:2869,Wheel Arrangement:0 - 4 - 2 T,Location:Snowdon Mountain Railway,Notes:Ralph ## Row 7, Built:1923,Number:8,Type:Mountain Railway Rack Steam Locomotive,Slm Number:2870,Wheel Arrangement:0 - 4 - 2 T,Location:Snowdon Mountain Railway,Notes:Eryri\n",
      "Using the information from the table, generate a paragraph-long summary to response to the following user query:\n",
      "Query: Summarize the basic information of the locomotive(s) built by Swiss Locomotive and Machine Works with slm number 988.\n",
      "\n",
      "Summary:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = generate_validate_prompt(validate_df[1])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9af6bbc-9da3-47e5-a9cd-154d320808a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 15/15 [06:11<00:00, 24.76s/it]\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"daryl149/llama-2-70b-chat-hf\"\n",
    "cache_dir='./llama2-70B_cache'\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_dir,\n",
    "                                        token=\"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\",\n",
    "                                         quantization_config=nf4_config,\n",
    "                                        device_map=\"auto\",\n",
    "                                        cache_dir=cache_dir\n",
    "                                        )\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_dir, \n",
    "                                           token=\"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\",\n",
    "                                           trust_remote_code=True, \n",
    "                                           cache_dir=cache_dir\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38fb50d3-97db-4ed3-ae03-650f7f83ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b22b354-c3ab-439f-83af-191c38a7419f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_summary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e2eb7ea-33c0-45b1-a6f1-987e52f1fe45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizer class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      " 66%|██████▌   | 132/200 [1:09:28<31:47, 28.05s/it]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "100%|██████████| 200/200 [1:46:38<00:00, 31.99s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(200)):\n",
    "    prompt = generate_validate_prompt(validate_df[i])\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful, respectful and honest assistant. Below is an instruction that describes a query-focused summarization task. Write a summary that appropriately response to the user query.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=400,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.0001,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    output_summary.append(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29f7b0ba-765b-4746-bab1-d9ef7d748d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten_table(table: Dict) -> str:\n",
    "    header = table.get('header', [])\n",
    "    rows = table.get('rows', [])\n",
    "    \n",
    "    flattened_rows = []\n",
    "    for i, row in enumerate(rows):\n",
    "        row_text = f\"Row {i}, \" + \",\".join([f\"{col}:{val}\" for col, val in zip(header, row)])\n",
    "        flattened_rows.append(\"## \" + row_text)\n",
    "\n",
    "    flattened_table = \" \".join(flattened_rows)\n",
    "    return flattened_table\n",
    "\n",
    "def generate_validate_prompt(examples):\n",
    "    table = examples['table']\n",
    "    query = examples['query']\n",
    "    summary = examples['summary']\n",
    "    table_title = table['title']\n",
    "    system_prompt = \"You are a helpful, respectful and honest assistant. Below is an instruction that describes a query-focused summarization task. Write a summary that appropriately response to the user query.\"\n",
    "    \n",
    "    task = \"Using the information from the table, generate a paragraph-long summary to response to the following user query:\"\n",
    "\n",
    "    \n",
    "    flattened_table = flatten_table(table)\n",
    "    input_text = f\"Table Title: {table_title}\\n{flattened_table}\\n{task}\\nQuery: {query}\\n\\nSummary:\\n\"\n",
    "    prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "{input_text} [/INST]\"\"\"\n",
    "    #prompt = f\"{system_prompt}\\n{input_text}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09b0495c-641a-413c-a003-841e6e8e1ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_summary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8699fb05-69f1-42a3-b739-3362e647de37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Below is an instruction that describes a query-focused summarization task. Write a summary that appropriately response to the user query.\n",
      "<</SYS>>\n",
      "Table Title: List of colleges and universities in Maine - Open institutions\n",
      "## Row 0, School:Bates College,Location(s):Lewiston,Control:Private,Type:Baccalaureate college,Enrollment (2016):1,903,Founded:1855 ## Row 1, School:Beal College,Location(s):Bangor,Control:Private (for-profit),Type:Associates college,Enrollment (2016):600,Founded:1891 ## Row 2, School:Bowdoin College,Location(s):Brunswick,Control:Private,Type:Baccalaureate college,Enrollment (2016):1,952,Founded:1794 ## Row 3, School:Central Maine Community College,Location(s):Auburn,Control:Public,Type:Associates college,Enrollment (2016):3,978,Founded:1963 ## Row 4, School:Colby College,Location(s):Waterville,Control:Private,Type:Baccalaureate college,Enrollment (2016):2,055,Founded:1813 ## Row 5, School:College of the Atlantic,Location(s):Bar Harbor,Control:Private,Type:Baccalaureate college,Enrollment (2016):424,Founded:1969 ## Row 6, School:Eastern Maine Community College,Location(s):Bangor,Control:Public,Type:Associates college,Enrollment (2016):4,051,Founded:1966 ## Row 7, School:Husson University,Location(s):Bangor and South Portland,Control:Private,Type:Masters university,Enrollment (2016):3,860,Founded:1898 ## Row 8, School:Institute for Doctoral Studies in the Visual Arts,Location(s):Portland,Control:Private,Type:Baccalaureate college,Enrollment (2016):79,Founded:2007 ## Row 9, School:Kennebec Valley Community College,Location(s):Fairfield,Control:Public,Type:Associates college,Enrollment (2016):3,284,Founded:1969 ## Row 10, School:Maine College of Art,Location(s):Portland,Control:Private,Type:School of art,Enrollment (2016):495,Founded:1882 ## Row 11, School:Maine Maritime Academy,Location(s):Castine,Control:Public,Type:Baccalaureate college,Enrollment (2016):1,037,Founded:1941 ## Row 12, School:Maine College of Health Professions,Location(s):Lewiston,Control:Private,Type:Associates college,Enrollment (2016):216,Founded:1891 ## Row 13, School:Northern Maine Community College,Location(s):Presque Isle,Control:Public,Type:Associates college,Enrollment (2016):1,285,Founded:1961 ## Row 14, School:Saint Joseph's College of Maine,Location(s):Standish,Control:Private,Type:Masters university,Enrollment (2016):4,465,Founded:1912 ## Row 15, School:Southern Maine Community College,Location(s):South Portland,Control:Public,Type:Associates college,Enrollment (2016):8,648,Founded:1946 ## Row 16, School:Thomas College,Location(s):Waterville,Control:Private,Type:Baccalaureate college,Enrollment (2016):1,918,Founded:1894 ## Row 17, School:Unity College,Location(s):Unity,Control:Private,Type:Baccalaureate college,Enrollment (2016):766,Founded:1965 ## Row 18, School:University of Maine,Location(s):Orono,Control:Public,Type:Research university,Enrollment (2016):12,488,Founded:1865 ## Row 19, School:University of Maine at Augusta,Location(s):Augusta,Control:Public,Type:Baccalaureate college,Enrollment (2016):6,193,Founded:1965 ## Row 20, School:University of Maine at Farmington,Location(s):Farmington,Control:Public,Type:Baccalaureate college,Enrollment (2016):2,424,Founded:1863 ## Row 21, School:University of Maine at Fort Kent,Location(s):Fort Kent,Control:Public,Type:Baccalaureate college,Enrollment (2016):2,032,Founded:1878 ## Row 22, School:University of Maine at Machias,Location(s):Machias,Control:Public,Type:Baccalaureate college,Enrollment (2016):1,007,Founded:1909 ## Row 23, School:University of Maine at Presque Isle,Location(s):Presque Isle,Control:Public,Type:Baccalaureate college,Enrollment (2016):1,657,Founded:1903 ## Row 24, School:University of New England,Location(s):Biddeford and Portland,Control:Private,Type:Masters university,Enrollment (2016):12,245,Founded:1939 ## Row 25, School:University of Southern Maine,Location(s):Gorham, Portland and Lewiston,Control:Public,Type:Masters university,Enrollment (2016):9,835,Founded:1878 ## Row 26, School:Washington County Community College,Location(s):Calais,Control:Public,Type:Associates college,Enrollment (2016):597,Founded:1969 ## Row 27, School:York County Community College,Location(s):Wells,Control:Public,Type:Associates college,Enrollment (2016):2,364,Founded:1994\n",
      "Using the information from the table, generate a paragraph-long summary to response to the following user query:\n",
      "Query: How many private versus public institutions are there and which category has a greater total enrollment in 2016?\n",
      "\n",
      "Summary:\n",
      " [/INST]\n"
     ]
    }
   ],
   "source": [
    "print(generate_validate_prompt(validate_df[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e2500a1-4047-4150-99f4-ef75b4dee471",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 77/200 [36:19<1:01:18, 29.91s/it]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "100%|██████████| 200/200 [1:34:02<00:00, 28.21s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(200)):\n",
    "    prompt = generate_validate_prompt(validate_df[i])\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful, respectful and honest assistant. Below is an instruction that describes a query-focused summarization task. Write a summary that appropriately response to the user query.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    # prompt = pipeline.tokenizer.apply_chat_template(\n",
    "    #         messages, \n",
    "    #         tokenize=False, \n",
    "    #         add_generation_prompt=True\n",
    "    # )\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=400,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.0001,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    final_summary.append(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f83d5743-69a1-4d2e-8f73-15a4b33d545d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.43802416271277406, 'rouge2': 0.22315055717637833, 'rougeL': 0.3339567177992043, 'rougeLsum': 0.3351979311861989} {'precision': [0.9254542589187622, 0.903687059879303, 0.892741858959198, 0.8382596969604492, 0.8795133233070374, 0.86527419090271, 0.8924967050552368, 0.9122763872146606, 0.873224139213562, 0.8648399114608765, 0.8779414892196655, 0.8244060277938843, 0.8873380422592163, 0.890691876411438, 0.8413833975791931, 0.9511193037033081, 0.8795099854469299, 0.8976823091506958, 0.8666635751724243, 0.904883861541748, 0.8972246646881104, 0.8855209350585938, 0.8773843050003052, 0.9489731192588806, 0.8841674327850342, 0.9214867353439331, 0.8996047973632812, 0.9254379272460938, 0.9023730754852295, 0.8697454929351807, 0.8328706622123718, 0.8654690980911255, 0.8901695013046265, 0.8708077073097229, 0.8705869317054749, 0.8608335256576538, 0.9153760671615601, 0.9522333741188049, 0.8936750888824463, 0.8720929622650146, 0.9089547395706177, 0.8584100008010864, 0.8707201480865479, 0.924479067325592, 0.9387384653091431, 0.8806690573692322, 0.9211636781692505, 0.8975293636322021, 0.8593438863754272, 0.8890705704689026, 0.8782400488853455, 0.9327939748764038, 0.9057838916778564, 0.9012045860290527, 0.8783629536628723, 0.8722383975982666, 0.8670471906661987, 0.8239976167678833, 0.8434712290763855, 0.8785351514816284, 0.8444464206695557, 0.8885930776596069, 0.8645700216293335, 0.8342170715332031, 0.9162824749946594, 0.8963104486465454, 0.8622924089431763, 0.8526854515075684, 0.8534443974494934, 0.8400183320045471, 0.8620424270629883, 0.8962578773498535, 0.9157181978225708, 0.9345921277999878, 0.9435945153236389, 0.8364866971969604, 0.883739173412323, 0.8882431387901306, 0.8708231449127197, 0.8991201519966125, 0.8972192406654358, 0.953280508518219, 0.8472570776939392, 0.855469286441803, 0.9353266954421997, 0.9338135719299316, 0.9333671927452087, 0.8631634712219238, 0.8659154176712036, 0.8784887790679932, 0.9538195729255676, 0.8960639834403992, 0.94157475233078, 0.9028086066246033, 0.8919682502746582, 0.8720791339874268, 0.8769575953483582, 0.8635895252227783, 0.9419105052947998, 0.8765581250190735, 0.8760972023010254, 0.8470734357833862, 0.8532552719116211, 0.9040412902832031, 0.895168662071228, 0.8715335130691528, 0.8896490335464478, 0.9532124996185303, 0.8909019231796265, 0.8713533282279968, 0.9533650875091553, 0.8637813329696655, 0.8952326774597168, 0.8853726983070374, 0.928918182849884, 0.8248394727706909, 0.8835771083831787, 0.9427851438522339, 0.8905373811721802, 0.8602798581123352, 0.8497152328491211, 0.9417520761489868, 0.8861261606216431, 0.8174720406532288, 0.8403711915016174, 0.9436686038970947, 0.9110028743743896, 0.8596859574317932, 0.8327837586402893, 0.8617650270462036, 0.892673134803772, 0.8667705655097961, 0.8833654522895813, 0.8940335512161255, 0.8746187090873718, 0.8597491979598999, 0.8572048544883728, 0.891162633895874, 0.8808478116989136, 0.8103318214416504, 0.9002892971038818, 0.8780653476715088, 0.9288707375526428, 0.888392448425293, 0.9044225215911865, 0.8710683584213257, 0.8924331665039062, 0.9202658534049988, 0.8440347909927368, 0.8640760183334351, 0.9105774164199829, 0.9133480191230774, 0.8775671124458313, 0.8697896003723145, 0.8636763095855713, 0.8687558770179749, 0.8947678208351135, 0.8932653665542603, 0.8218718767166138, 0.8570660352706909, 0.8508826494216919, 0.866089940071106, 0.8780720829963684, 0.9220442175865173, 0.9641895890235901, 0.932087779045105, 0.9574571847915649, 0.950309157371521, 0.9145479798316956, 0.9174556732177734, 0.9390107989311218, 0.8708310127258301, 0.9320379495620728, 0.8860343098640442, 0.9018162488937378, 0.8761414289474487, 0.8976173400878906, 0.8549058437347412, 0.8989913463592529, 0.891081690788269, 0.8946107029914856, 0.8684077262878418, 0.9158247113227844, 0.8974893093109131, 0.8885124921798706, 0.8614661693572998, 0.8914231061935425, 0.9208427667617798, 0.8844416737556458, 0.8858644962310791, 0.9486339092254639, 0.8492557406425476, 0.8647385239601135, 0.9265084862709045, 0.9636825919151306, 0.9170277118682861, 0.9357925057411194, 0.8560183048248291, 0.9060056805610657, 0.8605360984802246], 'recall': [0.884797990322113, 0.9407227039337158, 0.8798177242279053, 0.8024653196334839, 0.8954439759254456, 0.9327809810638428, 0.8490808010101318, 0.9218577146530151, 0.8717846870422363, 0.8962986469268799, 0.9095364809036255, 0.9176073670387268, 0.86606365442276, 0.872913658618927, 0.9160840511322021, 0.966166615486145, 0.8784075975418091, 0.862318217754364, 0.8862345218658447, 0.929196834564209, 0.9283554553985596, 0.9350187182426453, 0.8720923662185669, 0.9420335292816162, 0.878334641456604, 0.9108163118362427, 0.9196405410766602, 0.8702410459518433, 0.9256168603897095, 0.8706032037734985, 0.8240517377853394, 0.8763055801391602, 0.8875080943107605, 0.8911077976226807, 0.9120382070541382, 0.8955325484275818, 0.9212828278541565, 0.9164654016494751, 0.8865534663200378, 0.8874702453613281, 0.9351842999458313, 0.9283940196037292, 0.8606295585632324, 0.9347777962684631, 0.9602919816970825, 0.8720030784606934, 0.910937488079071, 0.8953027725219727, 0.8795692920684814, 0.9354175329208374, 0.8986396789550781, 0.9263580441474915, 0.8961472511291504, 0.900768518447876, 0.9154043197631836, 0.9065542221069336, 0.8635936975479126, 0.8829962611198425, 0.8571283221244812, 0.9241751432418823, 0.8706647157669067, 0.9269910454750061, 0.8548972010612488, 0.8567055463790894, 0.9282292723655701, 0.9344881176948547, 0.896977961063385, 0.8822813034057617, 0.8926303386688232, 0.8776544332504272, 0.8730393648147583, 0.8730109930038452, 0.927427351474762, 0.918588399887085, 0.9364423751831055, 0.8672605752944946, 0.9307342171669006, 0.8874610066413879, 0.9012713432312012, 0.974346399307251, 0.9621747136116028, 0.9354233145713806, 0.8888523578643799, 0.9522936344146729, 0.9206494092941284, 0.9355618357658386, 0.9459976553916931, 0.8945151567459106, 0.8942683935165405, 0.893498420715332, 0.9436900615692139, 0.9212460517883301, 0.8775908946990967, 0.8862167000770569, 0.9067273139953613, 0.8652759790420532, 0.8862969279289246, 0.9055493474006653, 0.8954207897186279, 0.8922564387321472, 0.9241656064987183, 0.8751126527786255, 0.902219295501709, 0.8817770481109619, 0.9141666889190674, 0.9233092665672302, 0.9123344421386719, 0.9652528166770935, 0.9385751485824585, 0.8903815150260925, 0.9072010517120361, 0.898209810256958, 0.9361487030982971, 0.8580223917961121, 0.9266349077224731, 0.8919800519943237, 0.897921621799469, 0.9273614883422852, 0.9196164608001709, 0.8851814270019531, 0.874839723110199, 0.9550117254257202, 0.9154601693153381, 0.8802326917648315, 0.9064056873321533, 0.9089946746826172, 0.9104506969451904, 0.9118738174438477, 0.8704825639724731, 0.9008178114891052, 0.9460636377334595, 0.9019612073898315, 0.8826741576194763, 0.8958933353424072, 0.8582651615142822, 0.8951871991157532, 0.868500292301178, 0.9002189636230469, 0.9226018786430359, 0.8883339166641235, 0.9033699631690979, 0.8855287432670593, 0.9127481579780579, 0.8698371648788452, 0.909691333770752, 0.8787282109260559, 0.9079106450080872, 0.9019604921340942, 0.8677793741226196, 0.8861404657363892, 0.8929957747459412, 0.8734052777290344, 0.926461398601532, 0.8686559200286865, 0.8594023585319519, 0.8689240217208862, 0.8983096480369568, 0.9300613403320312, 0.8505945205688477, 0.9303624629974365, 0.8687927722930908, 0.9217509031295776, 0.9171372652053833, 0.9445828199386597, 0.9679807424545288, 0.9454352855682373, 0.9613469839096069, 0.9244092702865601, 0.9183663129806519, 0.9280198812484741, 0.9669995307922363, 0.899716854095459, 0.8933005332946777, 0.9071851968765259, 0.9225387573242188, 0.8768081665039062, 0.8733927607536316, 0.8683281540870667, 0.8960683345794678, 0.9365904331207275, 0.9145535230636597, 0.875827431678772, 0.8907896280288696, 0.9294991493225098, 0.9119817018508911, 0.8823192715644836, 0.8345134258270264, 0.9227699041366577, 0.8736770153045654, 0.9323040843009949, 0.9471883773803711, 0.9184650182723999, 0.9476550221443176, 0.901874303817749, 0.9480147957801819, 0.8944029211997986, 0.9457584023475647, 0.8865313529968262, 0.9178062677383423, 0.8638278245925903], 'f1': [0.9046696424484253, 0.9218330979347229, 0.8862326145172119, 0.8199720978736877, 0.8874071836471558, 0.8977603316307068, 0.8702476024627686, 0.917042076587677, 0.8725038170814514, 0.880288302898407, 0.8934597373008728, 0.8685134649276733, 0.8765717148780823, 0.8817131519317627, 0.8771461844444275, 0.9585838913917542, 0.8789584040641785, 0.8796449899673462, 0.8763397932052612, 0.9168792366981506, 0.9125246405601501, 0.9095969200134277, 0.8747303485870361, 0.9454905390739441, 0.8812413811683655, 0.9161204695701599, 0.9095123410224915, 0.8969911336898804, 0.9138472080230713, 0.8701741695404053, 0.8284378051757812, 0.8708536028862, 0.8888368010520935, 0.8808408379554749, 0.8908306360244751, 0.8778402805328369, 0.9183199405670166, 0.9340070486068726, 0.8901000022888184, 0.8797143697738647, 0.9218829274177551, 0.8920314908027649, 0.8656454086303711, 0.929599940776825, 0.9493929147720337, 0.8763145804405212, 0.9160220623016357, 0.8964146971702576, 0.8693389892578125, 0.9116553664207458, 0.8883227109909058, 0.929564893245697, 0.9009397625923157, 0.9009864926338196, 0.8965011835098267, 0.8890653252601624, 0.8653169870376587, 0.8524773716926575, 0.85024493932724, 0.9007773995399475, 0.8573551774024963, 0.9073860049247742, 0.859706461429596, 0.8453117609024048, 0.9222171902656555, 0.9150012135505676, 0.8792932033538818, 0.8672309517860413, 0.8725976943969727, 0.8584240078926086, 0.8675060272216797, 0.8844817280769348, 0.9215355515480042, 0.9265211820602417, 0.940004825592041, 0.8515956997871399, 0.9066280722618103, 0.887851893901825, 0.885785698890686, 0.9352229833602905, 0.9285624027252197, 0.9442675113677979, 0.8675563931465149, 0.9012885093688965, 0.9279299974441528, 0.9346868991851807, 0.9396399855613708, 0.8785597085952759, 0.8798635601997375, 0.8859300017356873, 0.9487278461456299, 0.9084805250167847, 0.9084575772285461, 0.894435703754425, 0.899287223815918, 0.8686642646789551, 0.8816025257110596, 0.8840718865394592, 0.9180775284767151, 0.8843376040458679, 0.8994896411895752, 0.8608647584915161, 0.8770544528961182, 0.8927703499794006, 0.9045679569244385, 0.896674633026123, 0.900848925113678, 0.9591948390007019, 0.9141173958778381, 0.8807646632194519, 0.929710328578949, 0.8806592226028442, 0.9152336716651917, 0.8714830279350281, 0.9277751445770264, 0.8570969104766846, 0.8906916379928589, 0.9350097179412842, 0.9048433899879456, 0.8725530505180359, 0.8620944023132324, 0.9483355283737183, 0.9005542993545532, 0.8476923108100891, 0.8721402883529663, 0.9260072112083435, 0.9107266664505005, 0.8850111961364746, 0.8512158989906311, 0.8808587789535522, 0.9185932278633118, 0.8840157985687256, 0.8830196857452393, 0.89496248960495, 0.8663647174835205, 0.8771103620529175, 0.8628156185150146, 0.8956679105758667, 0.9012415409088135, 0.8475419282913208, 0.9018270373344421, 0.881781280040741, 0.9207388758659363, 0.8790168762207031, 0.907049298286438, 0.8748815059661865, 0.9001054167747498, 0.9110212326049805, 0.8557423949241638, 0.8749691247940063, 0.9017009139060974, 0.8929302096366882, 0.9013516306877136, 0.8692224025726318, 0.8615339994430542, 0.8688399195671082, 0.8965352773666382, 0.9112920761108398, 0.8359865546226501, 0.8922114372253418, 0.8597444295883179, 0.8930539488792419, 0.8971796035766602, 0.9331774711608887, 0.9660813808441162, 0.9387140870094299, 0.9593981504440308, 0.9371802806854248, 0.9164531230926514, 0.9227075576782227, 0.9527996182441711, 0.8850383162498474, 0.9122582077980042, 0.8964849710464478, 0.9120597839355469, 0.8764746785163879, 0.8853393793106079, 0.8615647554397583, 0.8975274562835693, 0.9132694602012634, 0.9044721722602844, 0.8721017837524414, 0.9031336903572083, 0.9132137894630432, 0.9000941514968872, 0.871768057346344, 0.8620299696922302, 0.9218053817749023, 0.8790264129638672, 0.9084911346435547, 0.9479106068611145, 0.8825055956840515, 0.9043000936508179, 0.9140254259109497, 0.9557844996452332, 0.9055740237236023, 0.9407490491867065, 0.8710076808929443, 0.9118677377700806, 0.8621788024902344], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.36.2)'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "rougeL = []\n",
    "bert = []\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "rougescore = evaluate.load(\"rouge\")\n",
    "\n",
    "bert_score = bertscore.compute(predictions=output_summary, references=validate_df['summary'], lang = \"en\")\n",
    "rouge_score = rougescore.compute(predictions=output_summary, references=validate_df['summary'])\n",
    "print(rouge_score, bert_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1828c039-5b53-4e52-8c9d-f14b2b891eb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.4694828438261428, 'rouge2': 0.24351902314368945, 'rougeL': 0.35012452007387795, 'rougeLsum': 0.35430039249642475} {'precision': [0.8953565359115601, 0.9415045380592346, 0.8889484405517578, 0.8771141171455383, 0.8301807641983032, 0.86527419090271, 0.8873572945594788, 0.8892030715942383, 0.8705712556838989, 0.8747484683990479, 0.8889113068580627, 0.9283400177955627, 0.8683232069015503, 0.8367645144462585, 0.8608748912811279, 0.8794569373130798, 0.8784754872322083, 0.9002668261528015, 0.883944034576416, 0.8790221214294434, 0.9294285774230957, 0.8510318994522095, 0.8952310681343079, 0.9245123863220215, 0.8624144792556763, 0.9232460260391235, 0.903176486492157, 0.9083459377288818, 0.8843892216682434, 0.8669808506965637, 0.862229585647583, 0.8774843215942383, 0.9186300039291382, 0.8728663921356201, 0.8645857572555542, 0.8689563274383545, 0.8672002553939819, 0.9004122018814087, 0.8707717657089233, 0.9491473436355591, 0.9213534593582153, 0.8747454285621643, 0.8796473741531372, 0.8552124500274658, 0.9387384653091431, 0.9080827832221985, 0.9323074221611023, 0.863821268081665, 0.8854531049728394, 0.8536167740821838, 0.8963292837142944, 0.9249783158302307, 0.9257282018661499, 0.944833517074585, 0.876561164855957, 0.7876673340797424, 0.8949081301689148, 0.8627628087997437, 0.8543943166732788, 0.8315383195877075, 0.8816124796867371, 0.9220960140228271, 0.8851029872894287, 0.887847900390625, 0.8807544112205505, 0.8929235935211182, 0.8775742053985596, 0.8896598219871521, 0.8681017160415649, 0.8835258483886719, 0.9002192616462708, 0.924726665019989, 0.9274725317955017, 0.9174976348876953, 0.9006794691085815, 0.8468445539474487, 0.8923588395118713, 0.8696507811546326, 0.8885685801506042, 0.8757572174072266, 0.8798173666000366, 0.9068528413772583, 0.8379837274551392, 0.8332936763763428, 0.9396242499351501, 0.8669761419296265, 0.9133564233779907, 0.8421741724014282, 0.868496298789978, 0.8982751369476318, 0.952221691608429, 0.9544366598129272, 0.8988913297653198, 0.8331577777862549, 0.8963046669960022, 0.8847446441650391, 0.8744951486587524, 0.8826630115509033, 0.9567121863365173, 0.8979654312133789, 0.8757354617118835, 0.8816855549812317, 0.8467376232147217, 0.9137787818908691, 0.917012095451355, 0.8884745240211487, 0.9353159666061401, 0.935001790523529, 0.9015083909034729, 0.8623858094215393, 0.8787897825241089, 0.8793408274650574, 0.8957818150520325, 0.8892651796340942, 0.9371956586837769, 0.8414593935012817, 0.8808619976043701, 0.9271979331970215, 0.9026460647583008, 0.8757277727127075, 0.9136924147605896, 0.9495523571968079, 0.8837628960609436, 0.83829665184021, 0.8505917191505432, 0.9429868459701538, 0.908054769039154, 0.8680611848831177, 0.890885055065155, 0.8540884256362915, 0.9482850432395935, 0.918816089630127, 0.8785563111305237, 0.8939111828804016, 0.8994219303131104, 0.8548795580863953, 0.8769727349281311, 0.8953976631164551, 0.8671950697898865, 0.7995684146881104, 0.8938112258911133, 0.8606595396995544, 0.90409255027771, 0.899824321269989, 0.9169358611106873, 0.8733827471733093, 0.9168433547019958, 0.9186294078826904, 0.8539188504219055, 0.8486577272415161, 0.9003620147705078, 0.8831413984298706, 0.8846240043640137, 0.8716637492179871, 0.8923157453536987, 0.886289119720459, 0.8891677856445312, 0.8833935856819153, 0.8359662294387817, 0.8645602464675903, 0.8243368864059448, 0.881406307220459, 0.8594805002212524, 0.9354519844055176, 0.9113207459449768, 0.9182225465774536, 0.9157513976097107, 0.9552044868469238, 0.9081490635871887, 0.918695867061615, 0.939010739326477, 0.8244518041610718, 0.9579244256019592, 0.9064838886260986, 0.9023644924163818, 0.8377505540847778, 0.8893793225288391, 0.8713119029998779, 0.9134156703948975, 0.8647725582122803, 0.9271345734596252, 0.8761326670646667, 0.9125403761863708, 0.901722252368927, 0.8518667817115784, 0.8832438588142395, 0.8846143484115601, 0.910494863986969, 0.8571207523345947, 0.9126952886581421, 0.9506909847259521, 0.8608005046844482, 0.8650045394897461, 0.9307469725608826, 0.9657371640205383, 0.9001885652542114, 0.863420844078064, 0.8471963405609131, 0.877677321434021, 0.8369102478027344], 'recall': [0.9230399131774902, 0.9543954730033875, 0.9113092422485352, 0.8925803303718567, 0.9111413359642029, 0.9327809810638428, 0.9060397148132324, 0.9122847318649292, 0.893478274345398, 0.8961122632026672, 0.8953429460525513, 0.9366801381111145, 0.8911731243133545, 0.8705245852470398, 0.9043054580688477, 0.9560564756393433, 0.8891004323959351, 0.8553639650344849, 0.9008610248565674, 0.9176543951034546, 0.9424658417701721, 0.8930208683013916, 0.8936717510223389, 0.9350038766860962, 0.8785673379898071, 0.9007474184036255, 0.9316617250442505, 0.9227169156074524, 0.9034277200698853, 0.9099956750869751, 0.820488452911377, 0.8711025714874268, 0.8948894143104553, 0.8891459703445435, 0.9300358891487122, 0.8975139856338501, 0.8927501440048218, 0.9154266119003296, 0.8710168600082397, 0.9344946146011353, 0.9290032982826233, 0.9388021230697632, 0.8793071508407593, 0.9298809766769409, 0.9602919816970825, 0.9198072552680969, 0.9193307161331177, 0.9080697298049927, 0.8812153339385986, 0.9173439741134644, 0.9204320311546326, 0.9168407320976257, 0.912248969078064, 0.9243547916412354, 0.9160543084144592, 0.90376216173172, 0.9164255857467651, 0.9372382164001465, 0.8979520797729492, 0.9189237356185913, 0.8755473494529724, 0.937393069267273, 0.9000930190086365, 0.9100216627120972, 0.9278533458709717, 0.9356878995895386, 0.8998078107833862, 0.9161027073860168, 0.8738160133361816, 0.9108890891075134, 0.8614941239356995, 0.9185124635696411, 0.9302821159362793, 0.91120445728302, 0.9285293221473694, 0.8641666173934937, 0.9366707801818848, 0.8969920873641968, 0.9063498973846436, 0.9667917490005493, 0.9425069093704224, 0.936782956123352, 0.888265073299408, 0.9461508989334106, 0.949432909488678, 0.9147871732711792, 0.901879072189331, 0.8745437860488892, 0.8981817960739136, 0.9188927412033081, 0.9419064521789551, 0.944445013999939, 0.928024411201477, 0.8807100057601929, 0.8906514644622803, 0.871239960193634, 0.887429416179657, 0.9191898107528687, 0.9420824646949768, 0.948482871055603, 0.927108645439148, 0.8606211543083191, 0.9046791195869446, 0.8940427303314209, 0.9138041734695435, 0.9243625998497009, 0.9148929119110107, 0.9442668557167053, 0.9328278303146362, 0.8869063258171082, 0.8887559771537781, 0.8783479928970337, 0.9484404921531677, 0.8909554481506348, 0.9296399354934692, 0.8767449259757996, 0.9008021950721741, 0.9110029935836792, 0.9314852952957153, 0.8894526958465576, 0.9277090430259705, 0.9597787857055664, 0.9289979338645935, 0.8838552832603455, 0.8942688703536987, 0.9102067947387695, 0.8999044895172119, 0.9083539247512817, 0.8985347151756287, 0.907891035079956, 0.960181474685669, 0.9153056740760803, 0.8855457305908203, 0.8959306478500366, 0.8786764144897461, 0.8875099420547485, 0.892827033996582, 0.9038704037666321, 0.9034943580627441, 0.8726528882980347, 0.9045459628105164, 0.8791303634643555, 0.9216236472129822, 0.8739598989486694, 0.9284840822219849, 0.9104747176170349, 0.9130382537841797, 0.8973339796066284, 0.8739292025566101, 0.8559930324554443, 0.8985507488250732, 0.9243859052658081, 0.9514593482017517, 0.8832303285598755, 0.8825581073760986, 0.8766375780105591, 0.9025311470031738, 0.9188843369483948, 0.8611329793930054, 0.9388316869735718, 0.8553215265274048, 0.9187914729118347, 0.9152419567108154, 0.9430992007255554, 0.9440230131149292, 0.9271389842033386, 0.9246633052825928, 0.9731655120849609, 0.9161773324012756, 0.9278101921081543, 0.9669995307922363, 0.874155580997467, 0.9502891302108765, 0.9163091778755188, 0.924528956413269, 0.8622371554374695, 0.8950717449188232, 0.8921765685081482, 0.912540078163147, 0.9102367162704468, 0.9289485216140747, 0.925532341003418, 0.9057998061180115, 0.9105638265609741, 0.9088784456253052, 0.8800099492073059, 0.8999714851379395, 0.925531804561615, 0.8612088561058044, 0.9258708953857422, 0.9449591636657715, 0.9167641401290894, 0.9463269710540771, 0.900677502155304, 0.9578052759170532, 0.8912158012390137, 0.9198237657546997, 0.8831976652145386, 0.9219419956207275, 0.8806363940238953], 'f1': [0.9089874625205994, 0.9479061961174011, 0.8999899625778198, 0.8847796320915222, 0.8687790036201477, 0.8977603316307068, 0.8966012001037598, 0.900596022605896, 0.8818760514259338, 0.8853015303611755, 0.8921155333518982, 0.9324914813041687, 0.8795998096466064, 0.853310763835907, 0.882055938243866, 0.9161583781242371, 0.8837559819221497, 0.8772412538528442, 0.8923223614692688, 0.897922933101654, 0.9359018206596375, 0.8715209364891052, 0.8944507241249084, 0.9297285676002502, 0.870415985584259, 0.9118579626083374, 0.9171980619430542, 0.915475070476532, 0.893807053565979, 0.8879676461219788, 0.8408413529396057, 0.8742818236351013, 0.9066042900085449, 0.8809309601783752, 0.8961173295974731, 0.8830043077468872, 0.8797897696495056, 0.907857358455658, 0.8708943128585815, 0.9417639970779419, 0.9251625537872314, 0.9056425094604492, 0.8794772624969482, 0.8909850120544434, 0.9493929147720337, 0.9139074087142944, 0.9257736802101135, 0.8853930234909058, 0.8833291530609131, 0.8843337297439575, 0.9082208275794983, 0.9208915829658508, 0.9189391136169434, 0.9344819784164429, 0.8958727121353149, 0.841730535030365, 0.9055390954017639, 0.898459792137146, 0.8756318688392639, 0.873049795627594, 0.8785694241523743, 0.9296815991401672, 0.8925350308418274, 0.8987980484962463, 0.9036905765533447, 0.9138056635856628, 0.8885519504547119, 0.902687668800354, 0.8709495067596436, 0.8969988226890564, 0.8804311156272888, 0.9216091632843018, 0.9288751482963562, 0.9143401980400085, 0.914392352104187, 0.8554179072380066, 0.9139780402183533, 0.8831099271774292, 0.8973711729049683, 0.9190255999565125, 0.9100838303565979, 0.9215749502182007, 0.8623921275138855, 0.8861434459686279, 0.9445030689239502, 0.8902401328086853, 0.9075814485549927, 0.8580538034439087, 0.8830896615982056, 0.9084669947624207, 0.9470360279083252, 0.9494145512580872, 0.9132255911827087, 0.8562741875648499, 0.8934691548347473, 0.8779404163360596, 0.8809148073196411, 0.900556206703186, 0.9493409991264343, 0.9225330948829651, 0.9006901383399963, 0.8710259795188904, 0.8747499585151672, 0.9038030505180359, 0.9154053330421448, 0.9060633182525635, 0.9249917268753052, 0.9396114349365234, 0.9169006943702698, 0.8744742274284363, 0.8837448358535767, 0.878844141960144, 0.9213593602180481, 0.8901094794273376, 0.9334024786949158, 0.8587398529052734, 0.890720546245575, 0.9190291166305542, 0.9168389439582825, 0.8825368881225586, 0.920647382736206, 0.9546381831169128, 0.9058160781860352, 0.8604733347892761, 0.8718835711479187, 0.9263069033622742, 0.9039612412452698, 0.8877506256103516, 0.8946935534477234, 0.8801683187484741, 0.9541961550712585, 0.9170575737953186, 0.8820372223854065, 0.8949196934700012, 0.8889281749725342, 0.8708891868591309, 0.8848289251327515, 0.8996140956878662, 0.8849726319313049, 0.8345135450363159, 0.8991464972496033, 0.869796872138977, 0.9127739071846008, 0.886703610420227, 0.9226738810539246, 0.8915430903434753, 0.9149368405342102, 0.907856822013855, 0.8638081550598145, 0.8523095846176147, 0.8994554281234741, 0.9032930731773376, 0.9168252348899841, 0.8774089813232422, 0.887410044670105, 0.8814369440078735, 0.8957996368408203, 0.9007894992828369, 0.8483629822731018, 0.9001665711402893, 0.8395434021949768, 0.8997106552124023, 0.88648521900177, 0.9392600059509277, 0.9273836612701416, 0.9226592183113098, 0.9201857447624207, 0.9641013145446777, 0.9121455550193787, 0.9232305288314819, 0.9527996182441711, 0.8485764265060425, 0.9540914297103882, 0.9113700985908508, 0.9133122563362122, 0.8498175144195557, 0.8922165036201477, 0.8816207647323608, 0.9129776358604431, 0.8869224190711975, 0.928040623664856, 0.900155246257782, 0.9091576337814331, 0.9061214327812195, 0.8794496059417725, 0.8816239833831787, 0.8922268152236938, 0.917951762676239, 0.8591598868370056, 0.91923588514328, 0.9478164315223694, 0.8879013657569885, 0.903840184211731, 0.9154653549194336, 0.9617548584938049, 0.8956796526908875, 0.8907303214073181, 0.864822506904602, 0.8992652297019958, 0.8582167029380798], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.36.2)'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "rougeL = []\n",
    "bert = []\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "rougescore = evaluate.load(\"rouge\")\n",
    "bert_score = bertscore.compute(predictions=final_summary, references=validate_df['summary'], lang = \"en\")\n",
    "rouge_score = rougescore.compute(predictions=final_summary, references=validate_df['summary'])\n",
    "print(rouge_score, bert_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a433c-26f3-4bf0-ba48-d19a6d28d5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-llm",
   "language": "python",
   "name": "hf-llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
