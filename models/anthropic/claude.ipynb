{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52999fc3-7020-4242-b9a2-d4698199eb4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99bf7d33-8952-4045-bc78-f420208490b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "claude_key = os.getenv(\"CLAUDE_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fa5cc09-bba1-481e-aac0-fbf1d9df1ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b833c5b9-d895-4986-b816-04ac5e3eefb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/cai6307/y.khan/conda/envs/hf-llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory ./QTSumm/validate not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSeq2SeqLM\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sample\n\u001b[0;32m---> 13\u001b[0m validate \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./QTSumm/validate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m validate\n",
      "File \u001b[0;32m/blue/cai6307/y.khan/conda/envs/hf-llm/lib/python3.10/site-packages/datasets/load.py:2244\u001b[0m, in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, fs, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2241\u001b[0m     path_join \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin\n\u001b[1;32m   2243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mexists(dest_dataset_path):\n\u001b[0;32m-> 2244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(path_join(dest_dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_INFO_FILENAME)) \u001b[38;5;129;01mand\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(\n\u001b[1;32m   2246\u001b[0m     path_join(dest_dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_STATE_JSON_FILENAME)\n\u001b[1;32m   2247\u001b[0m ):\n\u001b[1;32m   2248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory ./QTSumm/validate not found"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from datasets import load_dataset, DatasetDict, Dataset, concatenate_datasets\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import nltk\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from random import sample\n",
    "\n",
    "\n",
    "validate = datasets.load_from_disk(\"./QTSumm/validate\")\n",
    "validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721c096-7b2c-4e1d-aaa4-ef094d9354a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def to_pandas(item):\n",
    "  return pd.DataFrame(item['table'][\"rows\"],columns=item['table'][\"header\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c963d-8693-472b-9497-7395ae416e0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the client\n",
    "client = anthropic.Client(api_key = claude_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3b05bf9-7777-45b8-80be-d551217d3aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(table, question, title):\n",
    "    table_str = to_pandas(item).to_markdown()\n",
    "    prompt = f\"You are an AI assistant tasked with summarizing key points from tabular data based on a given query. The title of the table is {title}. \\n\\nTable data: \\n\\n{table_str}. \\n\\nQuery: \\n\\n{question}\\n\\nProvide a concise summary paragraph that captures the main points from the tabular data relevant to the given query.\" \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a2a5cc6-3917-4258-854f-e51aa1d8f5b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from anthropic import HUMAN_PROMPT, AI_PROMPT\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "summaries = []\n",
    "\n",
    "def generate_summary(item):\n",
    "    question = item['query']\n",
    "    title = item['table'][\"title\"]\n",
    "    table = to_pandas(item)\n",
    "    prompt = create_prompt(table, question, title)\n",
    "\n",
    "    # Call the completions API\n",
    "    completion = client.completions.create(\n",
    "            model=\"claude-v1\",\n",
    "            prompt = f\"{HUMAN_PROMPT} {prompt} {AI_PROMPT}\",\n",
    "            max_tokens_to_sample=500,\n",
    "    )\n",
    "\n",
    "    # Return the summary\n",
    "    return Markdown(completion.completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1dd90a7-1880-47e3-b760-f308dcdc5e1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "count = 1\n",
    "for item in validate:\n",
    "    print(count)\n",
    "    count += 1\n",
    "    summary = generate_summary(item)\n",
    "    summaries.append(summary)\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e732ecd-a5b8-4498-8c59-c1712ac45038",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Despite ranking in the bottom half of the Big Ten conference in 1919, a few teams had notable statistics. Minnesota finished with a 4-2-1 overall record and 3-2 conference record, ranking fourth in the conference. They scored 18.6 points per game while only allowing 6.4 points against per game, good for second and third in the conference respectively. Indiana and Purdue also had respectable overall records at 3-4 and 2-4-1, respectively, despite their lower rankings and struggling in conference play. Overall, some of the lower ranked teams in the 1919 Big Ten conference were able to achieve success and notable statistics outside of their conference performance and rankings."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba3eb085-50e1-4534-b799-c71ef9c5cb47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 200/200 [00:00<00:00, 19830.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# Convert Markdown objects to strings\n",
    "markdown_strings = [obj.data for obj in summaries]\n",
    "\n",
    "validate = validate.add_column(\"generated_summary\", markdown_strings)\n",
    "validate.save_to_disk(\"./QTSumm/claude-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24badafc-50a4-4ee0-9277-866693cebce7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vinu Mohan get Asianet award for Best New person for act as Mohan Krishnan in film Nivedyam. This movie come out in year 2007. It his second movie, after he first do in Kanne Madanguka in 2005.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate[10]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1ac81f3-ff8f-4c0b-aee2-c73a3976fa9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Vinu Mohan won the Asianet award for Best Newcomer for his role as Mohan Krishnan in the 2007 film Nivedyam.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate[10]['generated_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4edf2660-0a1f-48c0-95de-72f91d28fcb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "validate = datasets.load_from_disk(\"/home/y.khan/cai6307-y.khan/Query-Focused-Tabular-Summarization/data/claude/claude-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2793569-04ac-464a-a512-3511bfbf2ca9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 200/200 [00:00<00:00, 1704.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "example = validate.filter(lambda x: x[\"example_id\"] == \"1ae05e92-4ca6-483d-9307-488d28b0b31b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc53a536-7255-48ed-b696-8ae37e9e7d89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "suma = example['generated_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "673cc3c3-fd8b-45bd-ae0f-e9e8637ccb80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The players who attended Brigham Young University (Byu) and played for the Utah Jazz are:\n",
      "\n",
      "Andy Toolson, number 5, played as a guard and forward for the Jazz from 1990 to 1991 and again from 1995 to 1996.\n"
     ]
    }
   ],
   "source": [
    "print(suma[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0d0087f-5f80-48c3-870e-37ebbfe6c416",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "pred = validate['generated_summary']\n",
    "ref = validate['summary']\n",
    "sc = rouge.compute(predictions=pred, references=ref)\n",
    "bert_score = bertscore.compute(predictions=pred, references=ref, lang = \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fb9b891-54b8-4333-abe3-897694d3b081",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3711429403296882, 0.9011911398172379)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "sc['rougeLsum'], np.mean(bert_score['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2994a87c-e67c-490f-b0aa-6cc13f566559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb6ef2-e5f7-458f-8906-67c06e042df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf49ef7f-601e-4ec1-94ee-2e79f19786c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d3c87fe-796a-442d-a1ef-d5143e9eb5e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 23:21:35.454839: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-26 23:21:37.195324: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.4916664448292566, 'rouge2': 0.24649439518691946, 'rougeL': 0.3688335814276131, 'rougeLsum': 0.37027148460221343}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "rouge_results = rouge.compute(predictions=validate['generated_summary'], references=validate['summary'])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "785a0d0f-561b-45ad-99dd-d2f3418f69c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bertscore = evaluate.load(\"bertscore\")\n",
    "bert_score = bertscore.compute(predictions=validate['generated_summary'], references=validate['summary'], lang = \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c177f5fd-c521-484d-b272-59c232a8ebf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.895864452123642\n",
      "\n",
      "Recall: 0.907031309902668\n",
      "\n",
      "f1: 0.9011911398172379\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "precision = statistics.mean(bert_score['precision'])\n",
    "recall = statistics.mean(bert_score['recall'])\n",
    "f1 = statistics.mean(bert_score['f1'])\n",
    "print(\"Precision:\", precision)\n",
    "print(\"\\nRecall:\", recall)\n",
    "print(\"\\nf1:\", f1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86d084da-03f7-4cab-ae4a-0a08bc2a40ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.14202644137088632, 'precisions': [0.4123116563042626, 0.18172548575312977, 0.0968887870280181, 0.05604822349736278], 'brevity_penalty': 1.0, 'length_ratio': 1.328644786782764, 'translation_length': 17853, 'reference_length': 13437}\n"
     ]
    }
   ],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "bleu_result = bleu.compute(predictions=validate['generated_summary'], references=validate['summary'])\n",
    "print(bleu_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33397496-1795-4f63-a049-1afeed2f2960",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['precision', 'recall', 'f1', 'hashcode'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_score.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e8ec363-4e2c-4fd7-9b19-2ea06a07648b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'completion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrouge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rouge\n\u001b[1;32m      3\u001b[0m ref \u001b[38;5;241m=\u001b[39m validate[\u001b[38;5;241m4\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion\u001b[49m\u001b[38;5;241m.\u001b[39mcompletion\n\u001b[1;32m      5\u001b[0m rouge \u001b[38;5;241m=\u001b[39m Rouge()\n\u001b[1;32m      7\u001b[0m rouge_results \u001b[38;5;241m=\u001b[39m rouge\u001b[38;5;241m.\u001b[39mget_scores(pred, ref)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'completion' is not defined"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "ref = validate[4]['summary']\n",
    "pred = completion.completion\n",
    "rouge = Rouge()\n",
    "\n",
    "rouge_results = rouge.get_scores(pred, ref)\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37aaed72-9bb5-4714-9727-85df886acaa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rouge_scores_out = []\n",
    "for metric in [\"rouge-1\", \"rouge-2\", \"rouge-l\"]:\n",
    "    for label in [\"F-Score\"]:\n",
    "        eval_1_score = rouge_results[0][metric][label[0].lower()]\n",
    "\n",
    "        row = {\n",
    "            \"Metric\": f\"{metric} ({label})\",\n",
    "            \"Summary 1\": eval_1_score\n",
    "        }\n",
    "        rouge_scores_out.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "755b662a-e226-41bc-83e9-25129535c539",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Metric': 'rouge-1 (F-Score)', 'Summary 1': 0.7391304298416824},\n",
       " {'Metric': 'rouge-2 (F-Score)', 'Summary 1': 0.33846153353727815},\n",
       " {'Metric': 'rouge-l (F-Score)', 'Summary 1': 0.7173912994068998}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_scores_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "249a95ab-a128-437c-b4d3-56f04876d00d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 1 F1 Score: 0.9167320132255554\n"
     ]
    }
   ],
   "source": [
    "# BERTScore leverages the pre-trained contextual embeddings from BERT and matches words in candidate and reference sentences by cosine similarity.\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "# Instantiate the BERTScorer object for English language\n",
    "scorer = BERTScorer(lang=\"en\")\n",
    "\n",
    "# Calculate BERTScore for the summary 1 against the excerpt\n",
    "# P1, R1, F1_1 represent Precision, Recall, and F1 Score respectively\n",
    "P1, R1, F1_1 = scorer.score([pred], [ref])\n",
    "\n",
    "print(\"Summary 1 F1 Score:\", F1_1.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e87d380-7f7a-4b40-9ee2-d91431abf575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant tasked with summarizing key points from tabular data based on a given query. The title of the table is 2008–09 Stanford Cardinal women's basketball team - Schedule. \n",
      "\n",
      "Table data: \n",
      "\n",
      "|    | Date       | Location      | Opponent         |   Cardinal Points |   Opp. Points | Record   |\n",
      "|---:|:-----------|:--------------|:-----------------|------------------:|--------------:|:---------|\n",
      "|  0 | Nov. 14/08 | Stanford      | Minnesota        |                68 |            55 | 1-0      |\n",
      "|  1 | Nov. 16/08 | Waco, TX      | Baylor           |                65 |            81 | 1-1      |\n",
      "|  2 | Nov. 20/08 | Stanford      | New Mexico       |                84 |            46 | 2-1      |\n",
      "|  3 | Nov. 23/08 | Stanford      | Rutgers          |                81 |            47 | 3-1      |\n",
      "|  4 | Nov. 28/08 | Honolulu      | Purdue           |                78 |            70 | 4-1      |\n",
      "|  5 | Nov. 29/08 | Honolulu      | Iowa State       |                83 |            45 | 5-1      |\n",
      "|  6 | Nov. 30/08 | Honolulu      | Hawaii           |                83 |            54 | 6-1      |\n",
      "|  7 | Dec. 13/08 | Stanford      | Fresno State     |               100 |            62 | 7-1      |\n",
      "|  8 | Dec. 16/08 | Durham, NC    | Duke             |                52 |            56 | 7-2      |\n",
      "|  9 | Dec. 19/08 | Columbia, SC  | South Carolina   |                78 |            47 | 8-2      |\n",
      "| 10 | Dec. 21/08 | Knoxville, TN | Tennessee        |                69 |            79 | 8-3      |\n",
      "| 11 | Dec. 28/08 | Stanford      | UC Davis         |                84 |            49 | 9-3      |\n",
      "| 12 | Jan. 2/09  | Tempe, AZ     | Arizona State    |                64 |            61 | 9-3      |\n",
      "| 13 | Jan. 4/09  | Tucson, AZ    | Arizona          |                70 |            61 | 10-3     |\n",
      "| 14 | Jan. 8/09  | Stanford      | Washington       |               112 |            35 | 11-3     |\n",
      "| 15 | Jan. 10/09 | Stanford      | Washington State |               102 |            53 | 12-3     |\n",
      "| 16 | Jan. 18/09 | Berkeley, CA  | California       |                54 |            57 | 12-4     |\n",
      "| 17 | Jan. 22/09 | Eugene, OR    | Oregon           |                85 |            57 | 13-4     |\n",
      "| 18 | Jan. 24/09 | Corvallis, OR | Oregon State     |                69 |            54 | 14-4     |\n",
      "| 19 | Jan. 29/09 | Stanford      | USC              |                81 |            53 | 15-4     |\n",
      "| 20 | Feb. 1/09  | Stanford      | UCLA             |                68 |            51 | 16-4     |\n",
      "| 21 | Feb. 6/09  | Pullman, WA   | Washington State |                76 |            46 | 17-4     |\n",
      "| 22 | Feb. 8/09  | Seattle, WA   | Washington       |                76 |            54 | 18-4     |\n",
      "| 23 | Feb. 14/09 | Stanford      | California       |                58 |            41 | 19-4     |\n",
      "| 24 | Feb. 19/09 | Stanford      | Oregon State     |                72 |            43 | 20-4     |\n",
      "| 25 | Feb. 21/09 | Stanford      | Oregon           |                68 |            49 | 21-4     |\n",
      "| 26 | Feb. 27/09 | Los Angeles   | UCLA             |                69 |            58 | 22-4     |\n",
      "| 27 | Mar. 1/09  | Los Angeles   | USC              |                85 |            74 | 23-4     |\n",
      "| 28 | Mar. 5/09  | Stanford      | Arizona          |                70 |            67 | 24-4     |\n",
      "| 29 | Mar. 7/09  | Stanford      | Arizona State    |                77 |            68 | 25-4     |. \n",
      "\n",
      "Query: \n",
      "\n",
      "What were the top three games where the Stanford Cardinal women's basketball team scored the highest amount of points and against which opponents did they play in these games?\n",
      "\n",
      "Provide a concise summary paragraph that captures the main points from the tabular data relevant to the given query.\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a96a387-9a8f-49be-aaea-0efadc294175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the 2008-09 Stanford Cardinal women's basketball team schedule, the top three highest-scoring games for Stanford were:\n",
       "\n",
       "1. January 8, 2009 vs. Washington, where Stanford scored 112 points.\n",
       "2. January 10, 2009 vs. Washington State, with Stanford putting up 102 points.\n",
       "3. December 13, 2008 vs. Fresno State, in which Stanford scored 100 points.\n",
       "\n",
       "In these three games, Stanford dominated their opponents, winning by large margins of 77, 49, and 38 points respectively."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "Markdown(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c396a25a-2e61-4103-852a-1ca8098e44ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rouge-1': {'r': 0.6585365853658537, 'p': 0.46551724137931033, 'f': 0.5454545406019795}, 'rouge-2': {'r': 0.19298245614035087, 'p': 0.15492957746478872, 'f': 0.1718749950598146}, 'rouge-l': {'r': 0.5365853658536586, 'p': 0.3793103448275862, 'f': 0.4444444395918784}}]\n",
      "[{'Metric': 'rouge-1 (F-Score)', 'Summary 1': 0.5454545406019795}, {'Metric': 'rouge-2 (F-Score)', 'Summary 1': 0.1718749950598146}, {'Metric': 'rouge-l (F-Score)', 'Summary 1': 0.4444444395918784}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 1 F1 Score: 0.9067250490188599\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "ref = validate[12]['summary']\n",
    "pred = message.content[0].text\n",
    "rouge = Rouge()\n",
    "\n",
    "rouge_results = rouge.get_scores(pred, ref)\n",
    "print(rouge_results)\n",
    "\n",
    "rouge_scores_out = []\n",
    "for metric in [\"rouge-1\", \"rouge-2\", \"rouge-l\"]:\n",
    "    for label in [\"F-Score\"]:\n",
    "        eval_1_score = rouge_results[0][metric][label[0].lower()]\n",
    "\n",
    "        row = {\n",
    "            \"Metric\": f\"{metric} ({label})\",\n",
    "            \"Summary 1\": eval_1_score\n",
    "        }\n",
    "        rouge_scores_out.append(row)\n",
    "\n",
    "print(rouge_scores_out)\n",
    "\n",
    "# Instantiate the BERTScorer object for English language\n",
    "scorer = BERTScorer(lang=\"en\")\n",
    "\n",
    "# Calculate BERTScore for the summary 1 against the excerpt\n",
    "# P1, R1, F1_1 represent Precision, Recall, and F1 Score respectively\n",
    "P1, R1, F1_1 = scorer.score([pred], [ref])\n",
    "\n",
    "print(\"Summary 1 F1 Score:\", F1_1.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e3a065-9665-4cd3-be26-f78f572a30e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-llm",
   "language": "python",
   "name": "hf-llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
