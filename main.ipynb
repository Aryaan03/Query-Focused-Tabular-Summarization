{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c043447-4924-4dec-a4cb-f4b23c172aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import nltk\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "huggingface_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# If the dataset is gated/private, make sure you have run huggingface-cli login\n",
    "train_df = load_dataset(\"yale-nlp/QTSumm\", token = huggingface_token, split='train')\n",
    "test_df = load_dataset(\"yale-nlp/QTSumm\", token = huggingface_token, split='test')\n",
    "validate_df = load_dataset(\"yale-nlp/QTSumm\", token = huggingface_token, split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c39b834a-6e71-4f2a-9c76-3edfebcb88e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1052/1052 [00:01<00:00, 808.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def tokenization(examples):\n",
    "    inputs = [f\"query:  {query} header: {' '.join(map(str, entry.get('header', [])))} rows: {' '.join(map(str, entry.get('rows', [])))} title: {' '.join(map(str, entry.get('title', [])))}\"\n",
    "    for query, entry in zip(examples['query'], examples['table'])]\n",
    "    res = tokenizer(inputs, text_target=examples['summary'], truncation = True, padding = True)\n",
    "    return res\n",
    "\n",
    "tokenized_dataset_train = train_df.map(tokenization, batched=True)\n",
    "tokenized_dataset_test = test_df.map(tokenization, batched=True)\n",
    "tokenized_dataset_validate = validate_df.map(tokenization, batched=True)\n",
    "\n",
    "processed_data_train = tokenized_dataset_train.remove_columns(['table','summary', 'row_ids', 'example_id', 'query'])\n",
    "processed_data_test = tokenized_dataset_test.remove_columns(['table','summary', 'row_ids', 'example_id', 'query'])\n",
    "processed_data_validate = tokenized_dataset_validate.remove_columns(['table','summary', 'row_ids', 'example_id', 'query'])\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aed3aba-2da8-4690-a211-9eba7322cb25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1040' max='1040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1040/1040 33:31, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.148242</td>\n",
       "      <td>0.021399</td>\n",
       "      <td>0.003809</td>\n",
       "      <td>0.019011</td>\n",
       "      <td>0.018955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.599507</td>\n",
       "      <td>0.118695</td>\n",
       "      <td>0.044411</td>\n",
       "      <td>0.089383</td>\n",
       "      <td>0.089368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.975691</td>\n",
       "      <td>0.269453</td>\n",
       "      <td>0.144385</td>\n",
       "      <td>0.228142</td>\n",
       "      <td>0.228104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.862850</td>\n",
       "      <td>0.300123</td>\n",
       "      <td>0.166756</td>\n",
       "      <td>0.249949</td>\n",
       "      <td>0.249814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.818485</td>\n",
       "      <td>0.301852</td>\n",
       "      <td>0.168822</td>\n",
       "      <td>0.252331</td>\n",
       "      <td>0.252311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.796278</td>\n",
       "      <td>0.306688</td>\n",
       "      <td>0.172871</td>\n",
       "      <td>0.256442</td>\n",
       "      <td>0.256324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.782572</td>\n",
       "      <td>0.307781</td>\n",
       "      <td>0.173716</td>\n",
       "      <td>0.258493</td>\n",
       "      <td>0.258276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.773315</td>\n",
       "      <td>0.307163</td>\n",
       "      <td>0.174570</td>\n",
       "      <td>0.258990</td>\n",
       "      <td>0.258691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.765791</td>\n",
       "      <td>0.309006</td>\n",
       "      <td>0.175789</td>\n",
       "      <td>0.260155</td>\n",
       "      <td>0.259896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.867200</td>\n",
       "      <td>0.759892</td>\n",
       "      <td>0.307511</td>\n",
       "      <td>0.174005</td>\n",
       "      <td>0.259125</td>\n",
       "      <td>0.258862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.867200</td>\n",
       "      <td>0.755264</td>\n",
       "      <td>0.309316</td>\n",
       "      <td>0.176430</td>\n",
       "      <td>0.261205</td>\n",
       "      <td>0.260812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.867200</td>\n",
       "      <td>0.751509</td>\n",
       "      <td>0.310146</td>\n",
       "      <td>0.177567</td>\n",
       "      <td>0.261990</td>\n",
       "      <td>0.261747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.867200</td>\n",
       "      <td>0.749054</td>\n",
       "      <td>0.310016</td>\n",
       "      <td>0.177412</td>\n",
       "      <td>0.262472</td>\n",
       "      <td>0.262252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.867200</td>\n",
       "      <td>0.746698</td>\n",
       "      <td>0.311868</td>\n",
       "      <td>0.178947</td>\n",
       "      <td>0.264048</td>\n",
       "      <td>0.263837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.867200</td>\n",
       "      <td>0.744954</td>\n",
       "      <td>0.309403</td>\n",
       "      <td>0.177351</td>\n",
       "      <td>0.262458</td>\n",
       "      <td>0.262207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.867200</td>\n",
       "      <td>0.743177</td>\n",
       "      <td>0.311079</td>\n",
       "      <td>0.178762</td>\n",
       "      <td>0.263909</td>\n",
       "      <td>0.263677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.867200</td>\n",
       "      <td>0.742085</td>\n",
       "      <td>0.310508</td>\n",
       "      <td>0.178254</td>\n",
       "      <td>0.262395</td>\n",
       "      <td>0.262166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.867200</td>\n",
       "      <td>0.741304</td>\n",
       "      <td>0.310699</td>\n",
       "      <td>0.178630</td>\n",
       "      <td>0.263330</td>\n",
       "      <td>0.263059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.867200</td>\n",
       "      <td>0.740893</td>\n",
       "      <td>0.310887</td>\n",
       "      <td>0.178593</td>\n",
       "      <td>0.263437</td>\n",
       "      <td>0.263169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.714800</td>\n",
       "      <td>0.740804</td>\n",
       "      <td>0.311220</td>\n",
       "      <td>0.178722</td>\n",
       "      <td>0.263816</td>\n",
       "      <td>0.263498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1040, training_loss=1.749329554117643, metrics={'train_runtime': 2022.1609, 'train_samples_per_second': 49.264, 'train_steps_per_second': 0.514, 'total_flos': 6.821552745086976e+16, 'train_loss': 1.749329554117643, 'epoch': 20.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "\n",
    "def metric_fn(eval_predictions):\n",
    "    predictions, labels = eval_predictions\n",
    "    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    for label in labels:\n",
    "        label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    rouge = evaluate.load('rouge')\n",
    "    results = rouge.compute(predictions=decoded_predictions, references=decoded_labels)\n",
    "\n",
    "    return results\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model= model)\n",
    "\n",
    "train_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"./train_weights\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=64,\n",
    "        num_train_epochs=20,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        predict_with_generate=True,\n",
    "        overwrite_output_dir= True\n",
    "    )\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model,\n",
    "        train_args,\n",
    "        train_dataset=processed_data_train,\n",
    "        eval_dataset=processed_data_test,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=metric_fn\n",
    "    )\n",
    "    \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff6d3ad5-0dd5-49b9-8ee9-d8122a33cb75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./train_weights/checkpoint-1000\", device_map='auto')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./train_weights/checkpoint-1000\")\n",
    "\n",
    "pipe = pipeline(\"summarization\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "130b2a61-559f-45ae-ace1-6b75729b78a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_text(examples):\n",
    "    inputs = [f\"query:  {query} header: {' '.join(map(str, entry.get('header', [])))} rows: {' '.join(map(str, entry.get('rows', [])))} title: {' '.join(map(str, entry.get('title', [])))}\"\n",
    "    for query, entry in zip(examples['query'], examples['table'])]\n",
    "    examples['text'] = inputs\n",
    "    return examples\n",
    "\n",
    "tester = test_df.map(create_text, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bddb5c81-e186-4d50-a0d9-c892b546b0ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['row_ids', 'summary', 'query', 'table', 'example_id', 'text'],\n",
       "    num_rows: 1078\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f01fd5a9-953a-4f22-82d0-006901b3e0fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (958 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "vals = []\n",
    "for out in pipe(\n",
    "        KeyDataset(tester, \"text\"),\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id, ):\n",
    "    vals.append(out)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72a276af-afa9-425d-9a6e-342cc10374ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'The team that use KTM-VMC equipment is Danil Willemsen / Kenny Van Gaalen, who is driving the Zabel - Wsp team. He is a driver with a total of 452 points and a passenger with 385 points. Jan Hendrickx / Tim Smeuninx also use the KTM - VMC equipment, with 222 points and 369 points.'}]\n",
      "query:  Summarize the team(s) that are using KTM-VMC equipment. header: Position Driver / Passenger Equipment Bike No Points rows: ['1', 'Daniãl Willemsen / Kenny Van Gaalen', 'Zabel - Wsp', '1', '452'] ['2', 'Etienne Bax / Kaspars Stupelis', 'Zabel - Wsp', '5', '447'] ['3', 'Ben Adriaenssen / Sven Verbrugge', 'Ktm - Wsp', '6', '385'] ['4', 'Joris Hendrickx / Kaspars Liepins', 'Ktm - Vmc', '222', '369'] ['5', 'Jan Hendrickx / Tim Smeuninx', 'Zabel - Vmc', '3', '369'] ['6', 'Valentin Giraud / Nicolas Musset', 'Ktm - Wht', '138', '334'] ['7', 'Vaclav Rozehnal / Marek Rozehnal', 'Zabel - Vmc', '11', '240'] ['8', 'Marcel Willemsen / Gertie Eggink', 'Zabel - Mefo', '21', '223'] ['9', 'Maris Rupeiks / Elvijs Mucenieks', 'Zabel - Wsp', '4', '194'] title: S i d e c a r c r o s s   W o r l d   C h a m p i o n s h i p\n",
      "The team that is using KTM-VMC equipment in the Sidecarcross World Championship is in fourth place. Driver Joris Hendrickx and Passenger Kaspars Liepins are seated in this position with a total of 369 points within the championship.\n"
     ]
    }
   ],
   "source": [
    "i = 482\n",
    "print(pipe(tester['text'][i]))\n",
    "print(tester['text'][i])\n",
    "print(tester['summary'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "739ff8a9-5b06-48da-9093-45352aaa38a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'John Roberts from Maryland and Samuel Alito from New Zersey   were appointed by a President Bush.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester[0]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d8c606-08ba-4836-afc4-9c2e8e5f6d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HF_LLM",
   "language": "python",
   "name": "hf-llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
