{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a83f0215-637c-451a-b7c3-ab4ff759ad91",
   "metadata": {},
   "source": [
    "# Initialize Packages and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": null,
>>>>>>> 00ecabc2d139b545c72a437cb3639a591badd020
   "id": "378a909d-d189-4b51-82dd-6225b3ad5a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": null,
>>>>>>> 00ecabc2d139b545c72a437cb3639a591badd020
   "id": "a2c6c4ac-6ab9-42f2-8564-658d25a48355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from datasets import load_dataset, DatasetDict, Dataset, concatenate_datasets\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import nltk\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from random import sample\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": null,
>>>>>>> 00ecabc2d139b545c72a437cb3639a591badd020
   "id": "65c1c6cd-a76a-4931-bf35-f476dba482e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk('data/decomposed/decomposed_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10666512-9a9a-4b26-920d-950b1708d67f",
   "metadata": {},
   "source": [
    "# Check Test Dataset + Add Tokenizer Function"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": null,
>>>>>>> 00ecabc2d139b545c72a437cb3639a591badd020
   "id": "8f81cd10-0bf4-44df-97e8-0c8499f9fda8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset\n",
    "\n",
    "# Reduce it for testing\n",
    "random_indices = random.sample(range(len(dataset)), 5)\n",
    "dataset = dataset.select(random_indices)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": null,
>>>>>>> 00ecabc2d139b545c72a437cb3639a591badd020
   "id": "a8b60443-b398-4b09-9844-d9ea3dd8f7f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_predictions(examples, tokenizer, model):\n",
    "    generated_texts = []\n",
    "    for example in examples:\n",
    "        \n",
    "        # Intial tokenization\n",
    "        input_text = f\"query:  {example['query']} answer: {example['answers']} header: {' '.join(map(str, example['table'].get('header', [])))} rows: {' '.join(map(str, example['table'].get('rows', [])))} title: {' '.join(map(str, example['table'].get('title', [])))}\"\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate text and decode\n",
    "        output_sequences = model.generate(input_ids)\n",
    "        generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Add to list of generated text\n",
    "        generated_texts.append(generated_text)\n",
    "    \n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cb9766-242b-4971-a9c9-1ec51730c43d",
   "metadata": {},
   "source": [
    "# Load in models"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": null,
>>>>>>> 00ecabc2d139b545c72a437cb3639a591badd020
   "id": "e29af176-9328-48c7-ac10-13c0147eb8ba",
   "metadata": {
    "tags": []
   },
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
=======
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
>>>>>>> 00ecabc2d139b545c72a437cb3639a591badd020
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# gpt2\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
<<<<<<< HEAD
    "model_gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
=======
    "model_gpt2 = GPT2Model.from_pretrained(\"gpt2\")\n",
>>>>>>> 00ecabc2d139b545c72a437cb3639a591badd020
    "\n",
    "# t5 small\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# flan t5\n",
    "tokenizer_flant5 = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
<<<<<<< HEAD
    "model_flant5 = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n",
=======
    "model_flant5 = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")google/flan-t5-large\"\n",
>>>>>>> 00ecabc2d139b545c72a437cb3639a591badd020
    "\n",
    "# Bart\n",
    "tokenizer_bart = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model_bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": null,
>>>>>>> 00ecabc2d139b545c72a437cb3639a591badd020
   "id": "060a0a2d-3f1a-4161-aa4e-778afdebfc54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_and_tokenizers_with_names = [\n",
<<<<<<< HEAD
    "    # (\"GPT2\", tokenizer_gpt2, model_gpt2),\n",
=======
    "    (\"GPT2\", tokenizer_gpt2, model_gpt2),\n",
>>>>>>> 00ecabc2d139b545c72a437cb3639a591badd020
    "    (\"T5 Small\", tokenizer_t5, model_t5),\n",
    "    (\"FLAN-T5 Small\", tokenizer_flant5, model_flant5),\n",
    "    (\"BART Base\", tokenizer_bart, model_bart)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba72bb18-18e4-4788-b35a-737c53c497f5",
   "metadata": {},
   "source": [
    "# Make predictions using each Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": null,
>>>>>>> 00ecabc2d139b545c72a437cb3639a591badd020
   "id": "ca37a006-0548-4071-a55c-5da5a73413a5",
   "metadata": {
    "tags": []
   },
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: T5 Small\n",
      "Example 1: 1882\n",
      "Example 2: North Vernon\n",
      "Example 3: True\n",
      "Example 4: 2002\n",
      "Example 5: '2010', 'Morgan Lucas', ''] ['2009\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Model: FLAN-T5 Small\n",
      "Example 1: i n a t i - S p o r\n",
      "Example 2: a s t e r n d i a\n",
      "Example 3: o n f e r e n c e\n",
      "Example 4: q u e i r a M o s a\n",
      "Example 5: Maureen Murray\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Model: BART Base\n",
      "Example 1: query:  What year was the Cincinnati Reds baseball club founded and where do they commonly\n",
      "Example 2: query:  Summarize the basic information of the school(s) that left\n",
      "Example 3: query:  What is the correlation between the teams' AP final ranks and their P\n",
      "Example 4: query:  What is the extent in hectares of the largest and smallest protected areas located\n",
      "Example 5: query:  What were the ages of the winners of Miss New Hampshire Teen USA between\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> 00ecabc2d139b545c72a437cb3639a591badd020
   "source": [
    "for name, tokenizer, model in models_and_tokenizers_with_names:\n",
    "    print(f\"Model: {name}\")\n",
    "    predictions = generate_predictions(dataset, tokenizer, model)\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        print(f\"Example {i + 1}: {prediction}\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa9abf9-4b2d-4315-af37-0a140627a108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jairo_QTSUMM",
   "language": "python",
   "name": "jairo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
