{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a83f0215-637c-451a-b7c3-ab4ff759ad91",
   "metadata": {},
   "source": [
    "# Initialize Packages and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "378a909d-d189-4b51-82dd-6225b3ad5a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2c6c4ac-6ab9-42f2-8564-658d25a48355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from datasets import load_dataset, DatasetDict, Dataset, concatenate_datasets\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import nltk\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from random import sample\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65c1c6cd-a76a-4931-bf35-f476dba482e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk('data/decomposed/decomposed_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10666512-9a9a-4b26-920d-950b1708d67f",
   "metadata": {},
   "source": [
    "# Check Test Dataset + Add Tokenizer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b60443-b398-4b09-9844-d9ea3dd8f7f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_predictions(examples, tokenizer, model):\n",
    "    generated_texts = []\n",
    "    for example in examples:\n",
    "        \n",
    "        # Intial tokenization\n",
    "        input_text = f\"query:  {example['query']} answer: {example['answers']} header: {' '.join(map(str, example['table'].get('header', [])))} rows: {' '.join(map(str, example['table'].get('rows', [])))} title: {' '.join(map(str, example['table'].get('title', [])))}\"\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate text and decode\n",
    "        output_sequences = model.generate(input_ids)\n",
    "        generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Add to list of generated text\n",
    "        generated_texts.append(generated_text)\n",
    "    \n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90cc56cd-fca3-4534-b1c0-3e5c163e9745",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset\n",
    "\n",
    "# Reduce it for testing\n",
    "random_indices = random.sample(range(len(dataset)), 5)\n",
    "dataset = dataset.select(random_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cb9766-242b-4971-a9c9-1ec51730c43d",
   "metadata": {},
   "source": [
    "# Load in models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e29af176-9328-48c7-ac10-13c0147eb8ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# # gpt2\n",
    "# tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# model_gpt2 = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "# t5 small\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# flan t5\n",
    "tokenizer_flant5 = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model_flant5 = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "# Bart\n",
    "tokenizer_bart = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model_bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "060a0a2d-3f1a-4161-aa4e-778afdebfc54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_and_tokenizers_with_names = [\n",
    "    # (\"GPT2\", tokenizer_gpt2, model_gpt2),\n",
    "    (\"T5 Small\", tokenizer_t5, model_t5),\n",
    "    (\"FLAN-T5 Small\", tokenizer_flant5, model_flant5),\n",
    "    (\"BART Base\", tokenizer_bart, model_bart)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba72bb18-18e4-4788-b35a-737c53c497f5",
   "metadata": {},
   "source": [
    "# Make predictions using each Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca37a006-0548-4071-a55c-5da5a73413a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (760 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: T5 Small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (760 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: FLAN-T5 Small\n",
      "Model: BART Base\n"
     ]
    }
   ],
   "source": [
    "model_predictions = {}\n",
    "\n",
    "for name, tokenizer, model in models_and_tokenizers_with_names:\n",
    "    print(f\"Model: {name}\")\n",
    "    predictions = generate_predictions(dataset, tokenizer, model)\n",
    "    model_predictions[name] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5aa9abf9-4b2d-4315-af37-0a140627a108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T5 Small': [\"'59.941', '59.553', '59.5\", \"'Camden County Regiment', 'Edenton', '\", 'Russia (RUS)', 'John Putch', \"'2013', '2013', '2013', 'None\"], 'FLAN-T5 Small': ['e x a l l G r a n d', 'd e n t o n D i s t', '', 'i s t o f U g l y B', 't e r n'], 'BART Base': ['query:  Summarize the drivers that are part of the Minardi Team Us', 'query:  Who were the original commanders and what were their ranks for units that were', 'query:  What is the distribution of the 4 x 100 metres relay records among different', 'query:  Who were the directors and writers for the episode titled \"Backseat Betty', 'query:  Which school had the shortest membership duration in the Western Wayne Athletic Conference and']}\n"
     ]
    }
   ],
   "source": [
    "print(model_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6037d531-aa14-43e1-9242-38e38eee6aee",
   "metadata": {},
   "source": [
    "### Choosing Best Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a577b467-63b3-40e8-bfc2-9741f940e78e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "import numpy as np\n",
    "\n",
    "def select_best_guess(models_and_tokenizers_with_names, dataset, model_predictions, weights=(0.5, 0.5)):\n",
    "    weight_for_rouge, weight_for_bert = weights\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    best_guesses = []\n",
    "\n",
    "    for i, example in enumerate(dataset):\n",
    "        best_score = -np.inf\n",
    "        best_guess_info = {}\n",
    "        target_answer = example['query']\n",
    "        \n",
    "        for name, _, _ in models_and_tokenizers_with_names:\n",
    "            predictions = model_predictions[name][i]\n",
    "            \n",
    "            for prediction in predictions:\n",
    "                rouge_scores = scorer.score(target_answer, prediction)\n",
    "                rouge_score_avg = np.mean([rouge_scores['rouge1'].fmeasure, rouge_scores['rougeL'].fmeasure])\n",
    "                \n",
    "                _, _, bert_scores = score([prediction], [target_answer], lang=\"en\", verbose=False)\n",
    "                bert_score = bert_scores.mean().item()\n",
    "                \n",
    "                # Calculate combined score based on specified weights\n",
    "                combined_score = (weight_for_rouge * rouge_score_avg) + (weight_for_bert * bert_score)\n",
    "                \n",
    "                if combined_score > best_score:\n",
    "                    best_score = combined_score\n",
    "                    best_guess_info = {\n",
    "                        'model': name,\n",
    "                        'best_guess': prediction,\n",
    "                        'query': target_answer\n",
    "                    }\n",
    "        \n",
    "        best_guesses.append(best_guess_info)\n",
    "    \n",
    "    return best_guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03508c-f0b8-4ac2-a430-9a10382158a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_guesses = select_best_guess(models_and_tokenizers_with_names, dataset, model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4badbb8-6d8e-4a0c-bda9-20d9904cc0f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'BART Base', 'best_guess': 'M', 'query': 'Summarize the drivers that are part of the Minardi Team Usa Team.'}\n",
      "{'model': 'FLAN-T5 Small', 'best_guess': 'D', 'query': 'Who were the original commanders and what were their ranks for units that were created in 1775 and disbanded in 1783 under the Edenton District Brigade operation?'}\n",
      "{'model': 'BART Base', 'best_guess': '4', 'query': 'What is the distribution of the 4 x 100 metres relay records among different nations during the World Championships in Athletics from 1983 to 2015?'}\n",
      "{'model': 'BART Base', 'best_guess': 'k', 'query': 'Who were the directors and writers for the episode titled \"Backseat Betty\" and how many viewers did this episode attract?'}\n",
      "{'model': 'T5 Small', 'best_guess': '2', 'query': 'Which school had the shortest membership duration in the Western Wayne Athletic Conference and what were the circumstances surrounding its exit?'}\n"
     ]
    }
   ],
   "source": [
    "for guess in best_guesses:\n",
    "    print(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21709bb1-aee6-447f-b8c1-e33f8bea305f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jairo_QTSUMM",
   "language": "python",
   "name": "jairo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
