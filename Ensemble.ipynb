{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a83f0215-637c-451a-b7c3-ab4ff759ad91",
   "metadata": {},
   "source": [
    "# Initialize Packages and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "378a909d-d189-4b51-82dd-6225b3ad5a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2c6c4ac-6ab9-42f2-8564-658d25a48355",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 23:34:24.127208: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-21 23:34:28.241042: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from datasets import load_dataset, DatasetDict, Dataset, concatenate_datasets\n",
    "from bert_score import score as bert_score\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import nltk\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "from random import sample\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65c1c6cd-a76a-4931-bf35-f476dba482e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk('data/decomposed/decomposed_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10666512-9a9a-4b26-920d-950b1708d67f",
   "metadata": {},
   "source": [
    "# Check Test Dataset + Add Tokenizer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b60443-b398-4b09-9844-d9ea3dd8f7f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def tokenization_with_answer(examples, tokenizer):\n",
    "    inputs = []\n",
    "    \n",
    "    task_prefix = \"Given a query and a table, generate a summary that answers the query based on the information in the table: \"\n",
    "\n",
    "    for i, (query, table, summary) in enumerate(zip(examples['query'], examples['table'], examples['summary'])):\n",
    "        flattened_table = flatten_table(table, i)\n",
    "        input_text = f\"{task_prefix} Table {flattened_table}. Query: {query}\"\n",
    "\n",
    "        inputs.append(input_text)\n",
    "        \n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True,padding='max_length')\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=512, truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"] \n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def flatten_table(table: Dict, row_index: int) -> str:\n",
    "    header = table.get('header', [])\n",
    "    rows = table.get('rows', [])\n",
    "    title = table.get('title', [])\n",
    "\n",
    "    flattened_rows = []\n",
    "    for i, row in enumerate(rows):\n",
    "        row_text = f\"Row {i}, \" + \",\".join([f\"{col}:{val}\" for col, val in zip(header, row)])\n",
    "        flattened_rows.append(\"## \"+row_text)\n",
    "\n",
    "    flattened_table = f\"Title: {' '.join(map(str, title))}\" + \" \" + \" \".join(flattened_rows)\n",
    "    return flattened_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "339673bc-bebd-41e3-b059-1ec16f069869",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_predictions(examples, tokenizer, model):\n",
    "    generated_texts = []\n",
    "    for example in examples:\n",
    "        \n",
    "        # Intial tokenization\n",
    "        input_text = f\"query:  {example['query']} answer: {example['answers']} header: {' '.join(map(str, example['table'].get('header', [])))} rows: {' '.join(map(str, example['table'].get('rows', [])))} title: {' '.join(map(str, example['table'].get('title', [])))}\"\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate text and decode\n",
    "        output_sequences = model.generate(input_ids)\n",
    "        generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Add to list of generated text\n",
    "        generated_texts.append(generated_text)\n",
    "    \n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90cc56cd-fca3-4534-b1c0-3e5c163e9745",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['row_ids', 'table', 'summary', 'query', 'example_id', 'coordinates', 'answers'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset\n",
    "\n",
    "# Reduce it for testing\n",
    "# random_indices = random.sample(range(len(dataset)), 20)\n",
    "# dataset = dataset.select(random_indices)\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1568b21-3225-40bc-be09-59ac9b66ca68",
   "metadata": {},
   "source": [
    "## Trainer Creation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b73a465-f7f7-4842-8444-e03490207faf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def createTrainer(model, tokenzier):\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenzier, model)\n",
    "    \n",
    "    # Not needed, but trainer requires it even if not used\n",
    "    train_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"./train_weights_t5\",\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        num_train_epochs=1,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy = \"steps\",\n",
    "        eval_steps=200,\n",
    "        save_steps=200,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=5,\n",
    "        warmup_ratio=0.05,\n",
    "        load_best_model_at_end=True,\n",
    "        predict_with_generate=True,\n",
    "        overwrite_output_dir= True,\n",
    "        gradient_accumulation_steps = 2\n",
    "    )\n",
    "\n",
    "    return Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=train_args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cb9766-242b-4971-a9c9-1ec51730c43d",
   "metadata": {},
   "source": [
    "# Load in models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e29af176-9328-48c7-ac10-13c0147eb8ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "# # gpt2\n",
    "# tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# model_gpt2 = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "# t5 small\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(\"models/T5/T5-decomposed\")\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(\"models/T5/T5-decomposed\")\n",
    "\n",
    "# flan t5\n",
    "tokenizer_flant5 = T5Tokenizer.from_pretrained(\"Flan-T5\")\n",
    "model_flant5 = T5ForConditionalGeneration.from_pretrained(\"Flan-T5\")\n",
    "\n",
    "# Bart\n",
    "tokenizer_bart = BartTokenizer.from_pretrained(\"BART-Decomposed\")\n",
    "model_bart = BartForConditionalGeneration.from_pretrained(\"BART-Decomposed\")\n",
    "\n",
    "\n",
    "# LLaMA\n",
    "# tokenizer_llama = AutoTokenizer.from_pretrained(\"Llama\")\n",
    "# model_llama = AutoModelForMaskedLM.from_pretrained(\"Llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02e9b2d2-8790-4f97-8dd0-429a277441e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = \"Llama\"\n",
    "# llama = AutoModelForCausalLM.from_pretrained(\n",
    "#     base_model,\n",
    "#     token=\"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\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# llama.config.use_cache = False\n",
    "# llama.config.pretraining_tp = 1\n",
    "# tokenizer_llama = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True, \n",
    "#                                           token=\"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\",\n",
    "#                                          )\n",
    "# tokenizer_llama.pad_token = tokenizer_llama.eos_token\n",
    "# tokenizer_llama.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "060a0a2d-3f1a-4161-aa4e-778afdebfc54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_and_tokenizers_with_names = [\n",
    "   #  (\"GPT2\", tokenizer_gpt2, model_gpt2),\n",
    "    (\"T5\", tokenizer_t5, model_t5),\n",
    "    (\"FLAN-T5\", tokenizer_flant5, model_flant5),\n",
    "    (\"BART Base\", tokenizer_bart, model_bart),\n",
    "    # (\"LLaMA\", tokenizer_llama, model_llama)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba72bb18-18e4-4788-b35a-737c53c497f5",
   "metadata": {},
   "source": [
    "# Make predictions using each Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca37a006-0548-4071-a55c-5da5a73413a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: T5\n",
      "Model: FLAN-T5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: BART Base\n"
     ]
    }
   ],
   "source": [
    "model_predictions = {}\n",
    "\n",
    "for name, tokenizer, model in models_and_tokenizers_with_names:\n",
    "    print(f\"Model: {name}\")\n",
    "    predictions = generate_predictions(dataset, tokenizer, model)\n",
    "    model_predictions[name] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6037d531-aa14-43e1-9242-38e38eee6aee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Choosing Best Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a577b467-63b3-40e8-bfc2-9741f940e78e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "import numpy as np\n",
    "\n",
    "def select_best_guess(models_and_tokenizers_with_names, dataset, model_predictions, weights=(0.5, 0.5)):\n",
    "    weight_for_rouge, weight_for_bert = weights\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    best_guesses = []\n",
    "\n",
    "    for i, example in enumerate(dataset):\n",
    "        best_score = -np.inf\n",
    "        best_guess_info = {}\n",
    "        target_answer = example['summary']\n",
    "        \n",
    "        for name, _, _ in models_and_tokenizers_with_names:\n",
    "            prediction = model_predictions[name][i]\n",
    "            rouge_scores = scorer.score(target_answer, prediction)\n",
    "            rouge_score_avg = np.mean([rouge_scores['rouge1'].fmeasure, rouge_scores['rougeL'].fmeasure])\n",
    "\n",
    "            _, _, bert_scores = score([prediction], [target_answer], lang=\"en\", verbose=False)\n",
    "            bert_score = bert_scores.mean().item()\n",
    "\n",
    "            # Calculate combined score based on specified weights\n",
    "            combined_score = (weight_for_rouge * rouge_score_avg) + (weight_for_bert * bert_score)\n",
    "\n",
    "            if combined_score > best_score:\n",
    "                best_score = combined_score\n",
    "                best_guess_info = {\n",
    "                    'model': name,\n",
    "                    'best_guess': prediction,\n",
    "                    'answer': target_answer,\n",
    "                    'rouge': rouge_score_avg\n",
    "                }\n",
    "\n",
    "        best_guesses.append(best_guess_info)\n",
    "    \n",
    "    return best_guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03508c-f0b8-4ac2-a430-9a10382158a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "best_guesses = select_best_guess(models_and_tokenizers_with_names, dataset, model_predictions, (1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4badbb8-6d8e-4a0c-bda9-20d9904cc0f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_rogue = np.mean([guess['rouge'] for guess in best_guesses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5368d4a1-b454-4bba-99f9-a81105c30c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_rogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48595ca4-0e31-4ee0-86ed-3b45f3d5c49e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(best_guesses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jairo_QTSUMM",
   "language": "python",
   "name": "jairo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
