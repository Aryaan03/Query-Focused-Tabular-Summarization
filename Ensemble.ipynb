{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a83f0215-637c-451a-b7c3-ab4ff759ad91",
   "metadata": {},
   "source": [
    "# Initialize Packages and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a909d-d189-4b51-82dd-6225b3ad5a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c6c4ac-6ab9-42f2-8564-658d25a48355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from datasets import load_dataset, DatasetDict, Dataset, concatenate_datasets\n",
    "from bert_score import score as bert_score\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import nltk\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "from random import sample\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c1c6cd-a76a-4931-bf35-f476dba482e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk('data/decomposed/decomposed_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10666512-9a9a-4b26-920d-950b1708d67f",
   "metadata": {},
   "source": [
    "# Check Test Dataset + Add Tokenizer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b60443-b398-4b09-9844-d9ea3dd8f7f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def tokenization_with_answer(examples, tokenizer):\n",
    "    inputs = []\n",
    "    \n",
    "    task_prefix = \"Given a query and a table, generate a summary that answers the query based on the information in the table: \"\n",
    "\n",
    "    for i, (query, table, summary) in enumerate(zip(examples['query'], examples['table'], examples['summary'])):\n",
    "        flattened_table = flatten_table(table, i)\n",
    "        input_text = f\"{task_prefix} Table {flattened_table}. Query: {query}\"\n",
    "\n",
    "        inputs.append(input_text)\n",
    "        \n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True,padding='max_length')\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=512, truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"] \n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def flatten_table(table: Dict, row_index: int) -> str:\n",
    "    header = table.get('header', [])\n",
    "    rows = table.get('rows', [])\n",
    "    title = table.get('title', [])\n",
    "\n",
    "    flattened_rows = []\n",
    "    for i, row in enumerate(rows):\n",
    "        row_text = f\"Row {i}, \" + \",\".join([f\"{col}:{val}\" for col, val in zip(header, row)])\n",
    "        flattened_rows.append(\"## \"+row_text)\n",
    "\n",
    "    flattened_table = f\"Title: {' '.join(map(str, title))}\" + \" \" + \" \".join(flattened_rows)\n",
    "    return flattened_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339673bc-bebd-41e3-b059-1ec16f069869",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_predictions(examples, tokenizer, model):\n",
    "    generated_texts = []\n",
    "    for example in examples:\n",
    "        \n",
    "        # Intial tokenization\n",
    "        input_text = f\"query:  {example['query']} answer: {example['answers']} header: {' '.join(map(str, example['table'].get('header', [])))} rows: {' '.join(map(str, example['table'].get('rows', [])))} title: {' '.join(map(str, example['table'].get('title', [])))}\"\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate text and decode\n",
    "        output_sequences = model.generate(input_ids)\n",
    "        generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Add to list of generated text\n",
    "        generated_texts.append(generated_text)\n",
    "    \n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc56cd-fca3-4534-b1c0-3e5c163e9745",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset\n",
    "\n",
    "# Reduce it for testing\n",
    "random_indices = random.sample(range(len(dataset)), 20)\n",
    "dataset = dataset.select(random_indices)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1568b21-3225-40bc-be09-59ac9b66ca68",
   "metadata": {},
   "source": [
    "## Trainer Creation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73a465-f7f7-4842-8444-e03490207faf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def createTrainer(model, tokenzier):\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenzier, model)\n",
    "    \n",
    "    # Not needed, but trainer requires it even if not used\n",
    "    train_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"./train_weights_t5\",\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        num_train_epochs=1,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy = \"steps\",\n",
    "        eval_steps=200,\n",
    "        save_steps=200,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=5,\n",
    "        warmup_ratio=0.05,\n",
    "        load_best_model_at_end=True,\n",
    "        predict_with_generate=True,\n",
    "        overwrite_output_dir= True,\n",
    "        gradient_accumulation_steps = 2\n",
    "    )\n",
    "\n",
    "    return Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=train_args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cb9766-242b-4971-a9c9-1ec51730c43d",
   "metadata": {},
   "source": [
    "# Load in models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29af176-9328-48c7-ac10-13c0147eb8ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# # gpt2\n",
    "# tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# model_gpt2 = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "# t5 small\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# flan t5\n",
    "tokenizer_flant5 = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model_flant5 = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "# Bart\n",
    "tokenizer_bart = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model_bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a0a2d-3f1a-4161-aa4e-778afdebfc54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_and_tokenizers_with_names = [\n",
    "    # (\"GPT2\", tokenizer_gpt2, model_gpt2),\n",
    "    (\"T5 Small\", tokenizer_t5, model_t5),\n",
    "    (\"FLAN-T5 Small\", tokenizer_flant5, model_flant5),\n",
    "    (\"BART Base\", tokenizer_bart, model_bart)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba72bb18-18e4-4788-b35a-737c53c497f5",
   "metadata": {},
   "source": [
    "# Make predictions using each Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca37a006-0548-4071-a55c-5da5a73413a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_predictions = {}\n",
    "\n",
    "for name, tokenizer, model in models_and_tokenizers_with_names:\n",
    "    print(f\"Model: {name}\")\n",
    "    predictions = generate_predictions(dataset, tokenizer, model)\n",
    "    model_predictions[name] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5aa9abf9-4b2d-4315-af37-0a140627a108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T5 Small': ['Revelations', \"'13', '13', '23', '1\", 'Alan And Lia - Jay', \"'Purdue', '52', '56',\", 'True', \"'2', '1', '0', '0'\", 'Montgomery Renee Montgomery', \"'13,837,259', '2,724,975 (\", 'John Putch', 'True', 'True', 'True', '67, 67, 71, 75, 75, 75, 75, 75,', '121', \"'22'] ['Kansas State Capitol', '5'\", 'Miss New Hampshire', \"'3', 'October 4, 1953', 'Philadelphia Eagle\", \"'40', '.512', '24â€“17',\", '2015', \"'1949', '30,866']\"], 'FLAN-T5 Small': ['Revelations: The Revelations of Michael Hurst', '', 't a k e M e O u t ( U', 'a n f o r d', 't i s t i c s', 't e p h e n K i n g', 'o n e c t i c u t H', 'i n s u s u s e s', 'i s t o f U g l y B', 'o n f e r e a s o', 'w a t c h ( T v c h', 't a d i u m', 'a l C a n a d i e n', 'a t P u n j a b C r', '40 Corporate Woods', 's s N e w H a m p s', 'd u l e v e l a n', 's e a s o n s t a', 'o d e o C o n t e s', 'i s t o f u n i v'], 'BART Base': ['query:  Summarize the basic information of those episodes of Blood and Sand written', 'query:  Summarize the basic information of the game(s) between Buffalo', 'query:  What are the names of the couples who participated in the third episode of', \"query:  How many games did the Stanford Cardinal women's basketball team lose and which\", 'query:  Is there a significant variation between the highest rank (AP High) achieved', \"query:  Summarize Stephen King's performance in the 2008 season. answer:\", 'query:  Which player(s) scored more than 600 points in the 2008 -', 'query:  What trend can be observed in the Armenian population growth in percentage terms,', 'query:  Who were the directors and writers for the episode titled \"Backseat Betty', \"query:  What appears to be the correlation between the teams' rankings and their overall\", 'query:  Summarize the episodes of Alcatraz that were aired in 2012', 'query:  How did the attendance number vary with respect to different international Rugby League matches', 'query:  What can be inferred about the performance of the Montreal Canadiens in February 2007', 'query:  What is the average number of balls played by players who scored a century', 'query:  What is the tallest building in Kansas and how does its height and number', 'query:  Which hometown produced the most winners in the Miss New Hampshire Teen USA competition', 'query:  How many games did the Cleveland Browns win and lose during the 1953 regular', 'query:  What was the performance difference between the Milwaukee Bucks and the Cleveland Cavaliers during', 'query:  Which performer won the contest with the highest amount of points and in what', 'query:  Which university located in Bosnia and Herzegovina has the highest number']}\n"
     ]
    }
   ],
   "source": [
    "print(model_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6037d531-aa14-43e1-9242-38e38eee6aee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Choosing Best Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a577b467-63b3-40e8-bfc2-9741f940e78e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "import numpy as np\n",
    "\n",
    "def select_best_guess(models_and_tokenizers_with_names, dataset, model_predictions, weights=(0.5, 0.5)):\n",
    "    weight_for_rouge, weight_for_bert = weights\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    best_guesses = []\n",
    "\n",
    "    for i, example in enumerate(dataset):\n",
    "        best_score = -np.inf\n",
    "        best_guess_info = {}\n",
    "        target_answer = example['summary']\n",
    "        \n",
    "        for name, _, _ in models_and_tokenizers_with_names:\n",
    "            prediction = model_predictions[name][i]\n",
    "            rouge_scores = scorer.score(target_answer, prediction)\n",
    "            rouge_score_avg = np.mean([rouge_scores['rouge1'].fmeasure, rouge_scores['rougeL'].fmeasure])\n",
    "\n",
    "            _, _, bert_scores = score([prediction], [target_answer], lang=\"en\", verbose=False)\n",
    "            bert_score = bert_scores.mean().item()\n",
    "\n",
    "            # Calculate combined score based on specified weights\n",
    "            combined_score = (weight_for_rouge * rouge_score_avg) + (weight_for_bert * bert_score)\n",
    "\n",
    "            if combined_score > best_score:\n",
    "                best_score = combined_score\n",
    "                best_guess_info = {\n",
    "                    'model': name,\n",
    "                    'best_guess': prediction,\n",
    "                    'query': target_answer\n",
    "                }\n",
    "\n",
    "        best_guesses.append(best_guess_info)\n",
    "    \n",
    "    return best_guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b03508c-f0b8-4ac2-a430-9a10382158a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "best_guesses = select_best_guess(models_and_tokenizers_with_names, dataset, model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4badbb8-6d8e-4a0c-bda9-20d9904cc0f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'BART Base', 'best_guess': 'query:  Summarize the basic information of those episodes of Blood and Sand written', 'query': 'Brent Fletcher wrote for four episodes of Blood and Sand: Legends, Great and Unfortunate Things, Party Favors and Revelations. Directed by Grady Hall, Jesse Warn, Chris Martin - Jones and Michael Hurst respectively. Party Favors and Revelations aired on March 26th and April 9th of 2010 with Production Code numbers Sps110 and Sps112. They respectively achieved 1.27 million and 1.29 million views in the U.S. Great and Unfortunate Things also aired during March 2010 with production code Sps107 and 0.97 million views in the US while Legends was shown in February 2010 with Production Code Sps103, achieving a view count of 0.86million in the US.'}\n",
      "{'model': 'BART Base', 'best_guess': 'query:  Summarize the basic information of the game(s) between Buffalo', 'query': 'The Buffalo Bills faced the New England Patriots twice in their 1973 season. The first game was on September 16th, when the Bills beat the Patriots 31-13. The Bills had 23 first downs, while the Patriots managed 13. In the second game on December 9th, the Bills won even more convincingly, 37-13. The Bills had 13 first downs while the Patriots again managed 13.This gave the Bills a record at that time of 8-5.'}\n",
      "{'model': 'BART Base', 'best_guess': 'query:  What are the names of the couples who participated in the third episode of', 'query': 'The third episode of Take Me Out (Uk Game Show) aired on January 16th, 2010 and featured four couples. The first couple was Adam and Nicola, the second couple was David and Georgee, the third couple was Oliver and June, and the fourth couple was Tony and Jamie. The number of viewers for this episode were 4.74 million and it was ranked 18th out of all the ITV1 shows that week.'}\n",
      "{'model': 'BART Base', 'best_guess': \"query:  How many games did the Stanford Cardinal women's basketball team lose and which\", 'query': 'Stanford Cardinal women basketball team lose total 4 game in 2008-09 season. them play and lose to some team. Baylor, score 65 to 81 on November 16, 2008, in Waco, TX. also lose to Duke on December 16, 2008, in Durham, NC, score 52 to 56. another lose on December 21, 2008, against Tennessee in Knoxville, TN, final score 69 to 79. last lose against California on January 18, 2009, in Berkeley, CA, score 54 to 57.'}\n",
      "{'model': 'BART Base', 'best_guess': 'query:  Is there a significant variation between the highest rank (AP High) achieved', 'query': 'There be big different between top rank (AP High) get by some team in season and their final rank (AP final). For example, Illinois have number two top AP high rank but finish no rank in final place, and Wisconsin get number one AP high rank but finish in 11 place. This different show that some team have good start or best play in season but cannot keep same play always, so make their rank down when season finish.'}\n",
      "{'model': 'BART Base', 'best_guess': \"query:  Summarize Stephen King's performance in the 2008 season. answer:\", 'query': 'In the 2008 season, Stephen King played 20 games for the Chicago Fire of the United States. He scored 2 goals and made 1 assist while playing in these 20 games.'}\n",
      "{'model': 'BART Base', 'best_guess': 'query:  Which player(s) scored more than 600 points in the 2008 -', 'query': \"The 2008 - 09 Connecticut Huskies Women's Basketball Team had three players who scored more than 600 points: Maya Moore, Renee Montgomery and Tina Charles. Maya Moore scored 754 points, playing a total of 1209 minutes. She had 284 field goals, 90 three pointers, and 96 free throws. Montgomery Renee Montgomery scored 644 points in 1237 minutes, compiling 226 field goals, 99 three point shots and 93 free throws. Tina Charles scored 642 points in 982 minutes, with 259 field goals and 124 free throws.\"}\n",
      "{'model': 'BART Base', 'best_guess': 'query:  What trend can be observed in the Armenian population growth in percentage terms,', 'query': 'Between 1926 to 1989, overall Armenian people have big growth, the percent of Armenians in the total people increase from 84.5% to 93.3%. At same time, Azerbaijani people see decrease in their percent representation, drop from 9.4% in 1926 to 2.5% in 1989. Russian people also have decline in percent term, fall from 2.2% in 1926 to 1.5% in 1989. On other hand, Yazidis/Kurds people see increase in their percent representation, grow from 1.7% in 1926 to 1.6% in 1989.'}\n",
      "{'model': 'BART Base', 'best_guess': 'query:  Who were the directors and writers for the episode titled \"Backseat Betty', 'query': 'The episode with name \"Backseat Betty\" was direct by John Putch, writed by Tracy Poust and Jon Kinnally, and have 4.46 million people watch.'}\n",
      "{'model': 'BART Base', 'best_guess': \"query:  What appears to be the correlation between the teams' rankings and their overall\", 'query': \"The connection between teams' rankings and their all and conference records looks to be positive related. This means teams with higher rankings usually do better in their all and conference records. Teams that have higher rank, like Wisconsin and Purdue, have more good all records and conference records compare to teams with lower rank, like Indiana. Also, teams with higher rankings often have better points each game (PPG) and smaller points allowed each game (PAG) than lower-ranking teams. This tells that how the teams do in matches and their power to win games affect their all ranking straight, with better doing teams getting higher rankings.\"}\n",
      "{'model': 'BART Base', 'best_guess': 'query:  Summarize the episodes of Alcatraz that were aired in 2012', 'query': 'The episodes of Alcatraz that aired in 2012 include the pilot episode 1.01- Pilot, which achieved 1,299,000 viewers, 1.06 - Paxton Petty, which achieved 1,229,000 viewers, and 1.08 - The Ames Brothers, which achieved 1,193,000 viewers. All episodes aired in March, April, and May of 2012, respectively.'}\n",
      "{'model': 'BART Base', 'best_guess': 'query:  How did the attendance number vary with respect to different international Rugby League matches', 'query': 'The number of people for international Rugby League matches at Wembley Stadium from 1930 to 1997 had big changes. The lowest people was 9,874 in the 1973 Ashes series where Great Britain won against Australia, while the highest people was at the 1992 Rugby League World Cup Final, with 73,631 people watching Australia win against Great Britain. In all the years, matches with Australia and Great Britain usually had more people, like in the 1990 Ashes series with 54,569 people and the 1994 Ashes series with 57,034 people. But, there were times not like this, like the 1933-34 Kangaroo Tour match between Australia and Wales, which had much lower people of 10,000.'}\n",
      "{'model': 'BART Base', 'best_guess': 'query:  What can be inferred about the performance of the Montreal Canadiens in February 2007', 'query': 'In the February of 2007-08 season, Montreal Canadiens is do good, they win many games and get many point. When we see the table, they play 14 games total, and 9 is win. This show strong playing from team, because they can win almost two-third of games they play.  Moreover, Montreal Canadiens get 79 total point by end of February, mean they can get point always. This is can be proven by they never lose more than two game in row in this month. For teams they play with, the Canadiens have different kinds, like close fight with Ottawa Senators and Philadelphia Flyers. In general, Montreal Canadiens do very good in February of 2007-08 season by win games against many teams and get points all month.'}\n",
      "{'model': 'BART Base', 'best_guess': 'query:  What is the average number of balls played by players who scored a century', 'query': 'The average number of balls played by players who scored a century in a One Day International match at the Punjab Cricket Association IS Bindra Stadium is 130.4.'}\n",
      "{'model': 'BART Base', 'best_guess': 'query:  What is the tallest building in Kansas and how does its height and number', 'query': 'The tallest building in Kansas is Epic Center, it has 385 ft (117 m) tall. It has 22 floors and since 1989, it is tallest building in state. While it much taller than other buildings in list, it not have most number of floors. 250 Douglas Place in Wichita have that with 26 floors, but only 262 ft (80 m) height.'}\n",
      "{'model': 'BART Base', 'best_guess': 'query:  Which hometown produced the most winners in the Miss New Hampshire Teen USA competition', 'query': 'According to available data, Manchester produced most winners in Miss New Hampshire Teen USA competition, with total of 14 winners. This trend maybe shows Manchester has more big group of talented and accomplished young women, maybe because have more people or better chances for grow in confidence, skills and education. Another option, maybe because more organized or well-made pageant community or training program in city, which make contestants more prepared for succeed in competition.'}\n",
      "{'model': 'BART Base', 'best_guess': 'query:  How many games did the Cleveland Browns win and lose during the 1953 regular', 'query': 'In 1953 regular season, Cleveland Browns had good record, they win 11 games and only lost 1. Browns got wins from many different teams, like Green Bay Packers, Chicago Cardinals, Philadelphia Eagles, Washington Redskins, New York Giants, Pittsburgh Steelers, and San Francisco 49ers. In these games, they win with different points, from a close 23-21 win over 49ers to a big 62-14 win against Giants. They only lost come in the last game of regular season, when they play with Philadelphia Eagles for second time and were beaten with score 42-27. '}\n",
      "{'model': 'BART Base', 'best_guess': 'query:  What was the performance difference between the Milwaukee Bucks and the Cleveland Cavaliers during', 'query': 'In the 1981-82 season, Milwaukee Bucks had win percentage of .671, but Cleveland Cavaliers had much lower win percentage of .183. This show big performance different between two teams, with Milwaukee Bucks win much more games than Cleveland Cavaliers, by a margin of .488.'}\n",
      "{'model': 'BART Base', 'best_guess': 'query:  Which performer won the contest with the highest amount of points and in what', 'query': 'Coldplay won the contest with the highest amount of points, 673, in the year 2016.'}\n",
      "{'model': 'BART Base', 'best_guess': 'query:  Which university located in Bosnia and Herzegovina has the highest number', 'query': 'University of Sarajevo, it founded in 1949, have most high number students in Bosnia and Herzegovina, 30,866 students.'}\n"
     ]
    }
   ],
   "source": [
    "for guess in best_guesses:\n",
    "    print(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5368d4a1-b454-4bba-99f9-a81105c30c43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jairo_QTSUMM",
   "language": "python",
   "name": "jairo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
